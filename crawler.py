"""
crawler.py - ë‰´ìŠ¤ ìˆ˜ì§‘ & Google ë²ˆì—­ ëª¨ë“ˆ
ì¼ë³¸: Yahoo Japan News ê²€ìƒ‰ ê¸°ë°˜ (ë§¤ì²´ë³„ ê²€ìƒ‰ ëŒ€ì‹  í†µí•© ê²€ìƒ‰)
ì¤‘êµ­/ëŒ€ë§Œ: ë§¤ì²´ë³„ ê²€ìƒ‰
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re

# â”€â”€â”€ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€ í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",           "zh": "MUSINSA",    "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",    "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",       "zh": "Kç¾å¦†",      "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",                 "zh": "ç”µå•†",       "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",         "zh": "æ—¶å°š",       "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",            "zh": "é›¶å”®",       "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",         "zh": "ç¾å¦†",       "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",                "zh": "SPA",        "tw": "SPA"},
    "ëŸ­ì…”ë¦¬":      {"ja": "ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼",       "zh": "å¥¢ä¾ˆå“",     "tw": "å¥¢ä¾ˆå“"},
    "ì§€ì†ê°€ëŠ¥ì„±":  {"ja": "ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£",     "zh": "å¯æŒç»­å‘å±•",  "tw": "æ°¸çºŒç™¼å±•"},
}

# â”€â”€â”€ ë§¤ì²´ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    "japan": [
        {
            "name": "Yahoo Japan News",
            "url": "https://news.yahoo.co.jp",
            "search_url": "https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&sort=pub",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
            "parser": "yahoo_japan",   # ì „ìš© íŒŒì„œ ì§€ì •
        },
    ],
    "china": [
        {"name": "ç•Œé¢æ–°é—»",          "url": "https://www.jiemian.com",     "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",   "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª",              "url": "https://36kr.com",            "search_url": "https://36kr.com/search/articles/{keyword}",               "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "äº¿é‚¦åŠ¨åŠ›",          "url": "https://www.ebrun.com",       "search_url": "https://www.ebrun.com/search/?q={keyword}",                "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD Greater China", "url": "https://wwdgreaterchina.com", "search_url": "https://wwdgreaterchina.com/?s={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Vogue China",       "url": "https://www.vogue.com.cn",    "search_url": "https://www.vogue.com.cn/search?q={keyword}",              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "ç¬¬ä¸€è´¢ç»",          "url": "https://www.yicai.com",       "search_url": "https://www.yicai.com/search/?keys={keyword}",             "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "èµ¢å•†ç½‘",            "url": "https://m.winshang.com",      "search_url": "https://m.winshang.com/search.html?keyword={keyword}",     "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ–°æµª",              "url": "https://www.sina.com.cn",     "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co",           "url": "https://luxe.co",             "search_url": "https://luxe.co/?s={keyword}",                             "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        {"name": "æ•¸ä½æ™‚ä»£", "url": "https://www.bnext.com.tw", "search_url": "https://www.bnext.com.tw/search/{keyword}",  "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
        {"name": "å·¥å•†æ™‚å ±", "url": "https://www.ctee.com.tw",  "search_url": "https://www.ctee.com.tw/search?q={keyword}", "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
    ],
}

# â”€â”€â”€ ë‚ ì§œ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DATE_PATTERNS = [
    (r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", "%Y-%m-%dT%H:%M:%S"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", "%Y-%m-%d %H:%M:%S"),
    (r"\d{4}/\d{2}/\d{2} \d{2}:\d{2}",        "%Y/%m/%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}",        "%Y-%m-%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2}",                    "%Y-%m-%d"),
    (r"\d{4}/\d{2}/\d{2}",                    "%Y/%m/%d"),
    (r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",            "%Yå¹´%mæœˆ%dæ—¥"),
]

def parse_date(text: str):
    """
    ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±. ì•¼í›„ ì¬íŒ¬ì˜ ìƒëŒ€ì‹œê°„(Nåˆ†å‰, Næ™‚é–“å‰, æ˜¨æ—¥)ë„ ì§€ì›.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    if not text:
        return None
    text = text.strip()
    now = datetime.now()

    # ì•¼í›„ ì¬íŒ¬ ìƒëŒ€ì‹œê°„ ì²˜ë¦¬
    if "åˆ†å‰" in text:
        m = re.search(r"(\d+)", text)
        return now - timedelta(minutes=int(m.group(1))) if m else now
    if "æ™‚é–“å‰" in text:
        m = re.search(r"(\d+)", text)
        return now - timedelta(hours=int(m.group(1))) if m else now
    if "æ˜¨æ—¥" in text:
        return now - timedelta(days=1)
    if "ä»Šæ—¥" in text or "ãŸã£ãŸä»Š" in text:
        return now

    # ì¼ë°˜ ë‚ ì§œ íŒ¨í„´
    text = re.sub(r"[+Z]\d{2}:?\d{0,2}$", "", text)
    for pattern, fmt in DATE_PATTERNS:
        m = re.search(pattern, text)
        if m:
            try:
                return datetime.strptime(m.group(0), fmt)
            except ValueError:
                continue
    return None


def clean_text(text: str) -> str:
    """HTML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = (text.replace("&amp;", "&").replace("&lt;", "<")
                .replace("&gt;", ">").replace("&nbsp;", " ").replace("&quot;", '"'))
    return re.sub(r"\s+", " ", text).strip()


# â”€â”€â”€ Google ë²ˆì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text or not text.strip():
        return text
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS,
        )
        resp.raise_for_status()
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or text
    except Exception:
        return text


def translate_keyword_to_lang(keyword_ko: str, lang: str) -> str:
    """í‚¤ì›Œë“œë¥¼ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­ (ì‚¬ì „ ìš°ì„  â†’ Google ë²ˆì—­ fallback)"""
    if keyword_ko in KEYWORD_TRANSLATIONS:
        return KEYWORD_TRANSLATIONS[keyword_ko].get(lang, keyword_ko)
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": "ko", "tl": lang_map.get(lang, "ja"), "dt": "t", "q": keyword_ko},
            timeout=10, headers=HEADERS,
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or keyword_ko
    except Exception:
        return keyword_ko


# â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch(self, url: str, timeout: int = 15):
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "html.parser")
        except Exception:
            return None

    def is_within_cutoff(self, date_str: str) -> bool:
        """ë‚ ì§œë¥¼ ëª…í™•íˆ íŒŒì‹±í•œ ê²½ìš°ì—ë§Œ í¬í•¨ (ë‚ ì§œ ë¶ˆëª… â†’ False)"""
        dt = parse_date(date_str)
        if dt is None:
            return False
        return dt >= self.cutoff

    # â”€â”€ ì•¼í›„ ì¬íŒ¬ ì „ìš© íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_yahoo_japan(self, soup: BeautifulSoup) -> list[dict]:
        """
        Yahoo Japan News ê²€ìƒ‰ ê²°ê³¼ ì „ìš© íŒŒì„œ.
        li.sw-Card ì…€ë ‰í„° ê¸°ë°˜ìœ¼ë¡œ ì œëª©/URL/ë‚ ì§œ/ë§¤ì²´ëª… ì¶”ì¶œ.
        ì…€ë ‰í„°ê°€ ë§ì§€ ì•Šì„ ê²½ìš° parse_genericìœ¼ë¡œ ìë™ fallback.
        """
        results = []

        # ì•¼í›„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì¹´ë“œ (PC/ëª¨ë°”ì¼ ê³µí†µ ì…€ë ‰í„°)
        items = soup.select("li.sw-Card, div.sw-Card, article.sw-Card")

        if not items:
            # ì…€ë ‰í„° ë³€ê²½ì— ëŒ€ë¹„í•œ fallback
            return self.parse_generic(soup, "https://news.yahoo.co.jp")

        for item in items:
            # ì œëª©
            title_tag = (
                item.select_one("h3.sw-Card__title") or
                item.select_one("h2.sw-Card__title") or
                item.select_one("[class*='title']")
            )
            # ë§í¬
            a_tag = (
                item.select_one("a.sw-Card__titleInner") or
                item.select_one("a[href*='news.yahoo.co.jp']") or
                item.find("a", href=True)
            )
            # ë‚ ì§œ
            date_tag = (
                item.select_one("span.sw-Card__time") or
                item.select_one("time") or
                item.select_one("[class*='time'], [class*='date']")
            )
            # ë§¤ì²´ëª…
            source_tag = (
                item.select_one("span.sw-Card__sender") or
                item.select_one("[class*='sender'], [class*='source'], [class*='media']")
            )

            if not title_tag or not a_tag:
                continue

            title = clean_text(title_tag.get_text())
            url   = a_tag.get("href", "")
            if not url or len(title) < 5:
                continue

            # ë‚ ì§œ ì¶”ì¶œ (datetime ì†ì„± ìš°ì„ , ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸)
            if date_tag:
                date_str = date_tag.get("datetime") or date_tag.get_text(strip=True)
            else:
                date_str = ""

            if not self.is_within_cutoff(date_str):
                continue

            media_name = clean_text(source_tag.get_text()) if source_tag else "Yahoo Japan"

            results.append({
                "title":    title,
                "url":      url,
                "date":     date_str,
                "media":    media_name,   # ì›ë³¸ ë§¤ì²´ëª… ë³„ë„ ë³´ì¡´
            })

        return results[:20]

    # â”€â”€ ë²”ìš© íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _find_date_in_tag(self, tag) -> str:
        """íƒœê·¸ ë‚´ ë‚ ì§œ ë¬¸ìì—´ ì¶”ì¶œ (3ë‹¨ê³„ ìˆœì°¨ ì‹œë„)"""
        t = tag.find("time")
        if t:
            candidate = t.get("datetime") or t.get_text(strip=True)
            if parse_date(candidate):
                return candidate

        d = tag.find(True, class_=re.compile(r"\b(date|time|pub|posted|created|updated)\b", re.I))
        if d:
            candidate = d.get("datetime") or d.get_text(strip=True)
            if parse_date(candidate):
                return candidate

        raw = tag.get_text(" ", strip=True)
        for pattern, _ in DATE_PATTERNS:
            m = re.search(pattern, raw)
            if m:
                return m.group(0)

        return ""

    def parse_generic(self, soup: BeautifulSoup, base_url: str) -> list[dict]:
        """ë²”ìš© íŒŒì„œ - article > híƒœê·¸ + aë§í¬ ê¸°ë°˜"""
        candidates = []
        seen = set()

        # (A) <article> íƒœê·¸ ê¸°ë°˜
        for article in soup.find_all("article"):
            title_tag = article.find(["h1", "h2", "h3", "h4"])
            a_tag     = article.find("a", href=True)
            if not title_tag or not a_tag:
                continue
            title = clean_text(title_tag.get_text())
            url   = urljoin(base_url, a_tag["href"])
            date  = self._find_date_in_tag(article)
            if url not in seen and len(title) > 5:
                seen.add(url)
                candidates.append({"title": title, "url": url, "date": date})

        # (B) h2/h3 ê¸°ë°˜ fallback
        if not candidates:
            for h in soup.find_all(["h2", "h3"]):
                a_tag = h.find("a", href=True) or h.find_parent("a", href=True)
                if not a_tag:
                    continue
                title  = clean_text(h.get_text())
                url    = urljoin(base_url, a_tag["href"])
                parent = h.find_parent(["li", "div", "section"])
                date   = self._find_date_in_tag(parent) if parent else ""
                if url not in seen and len(title) > 5:
                    seen.add(url)
                    candidates.append({"title": title, "url": url, "date": date})

        # ì—„ê²©í•œ ë‚ ì§œ í•„í„°
        return [c for c in candidates if self.is_within_cutoff(c["date"])][:12]

    # â”€â”€ ë§¤ì²´ ê²€ìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def search_source(self, source: dict, keyword: str) -> list[dict]:
        soup = self.fetch(source["search_url"].format(keyword=quote(keyword)))
        if not soup:
            return []

        # ë§¤ì²´ë³„ ì „ìš© íŒŒì„œ ì„ íƒ
        parser_name = source.get("parser", "generic")
        if parser_name == "yahoo_japan":
            results = self.parse_yahoo_japan(soup)
        else:
            results = self.parse_generic(soup, source["url"])

        # URL íŒ¨í„´ í•„í„° (senken ë“± ì¹´í…Œê³ ë¦¬ ë§í¬ ì œê±°)
        exclude_patterns   = source.get("exclude_url_patterns", [])
        exclude_exact_urls = set(source.get("exclude_exact_urls", []))
        if exclude_patterns or exclude_exact_urls:
            before  = len(results)
            results = [
                r for r in results
                if r.get("url", "") not in exclude_exact_urls
                and not any(p in r.get("url", "") for p in exclude_patterns)
            ]
            if before - len(results):
                print(f"  [{source['name']}] ë¹„ê¸°ì‚¬ ë§í¬ {before - len(results)}ê±´ ì œì™¸")

        # ê³µí†µ ë©”íƒ€ ì¶”ê°€
        for r in results:
            r.setdefault("source",     source["name"])
            r.setdefault("source_url", source["url"])
            r.setdefault("language",   source["language"])
            r.setdefault("flag",       source.get("flag", ""))

        return results

    def crawl_category(self, category: str, keyword_ko: str, progress_callback=None) -> list[dict]:
        all_articles = []
        for source in SOURCES.get(category, []):
            keyword = translate_keyword_to_lang(keyword_ko, source["language"])
            if progress_callback:
                progress_callback(source["name"], keyword)
            all_articles.extend(self.search_source(source, keyword))
            time.sleep(0.8)
        return all_articles


# â”€â”€â”€ ë²ˆì—­ ì¼ê´„ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_articles(articles: list[dict], progress_callback=None) -> list[dict]:
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    for i, a in enumerate(articles):
        if progress_callback:
            progress_callback(i + 1, len(articles), a.get("source", ""))
        src = lang_map.get(a.get("language", "ja"), "auto")
        a["title_ko"] = translate_to_korean(a["title"], src_lang=src)
        time.sleep(0.3)
    return articles


# â”€â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    keyword_ko: str,
    days: int = 7,
    active_categories: list = None,
    on_status=None,
    on_progress=None,
) -> dict:
    """
    ìˆ˜ì§‘ â†’ ë²ˆì—­ íŒŒì´í”„ë¼ì¸.
    ë°˜í™˜ê°’: {"musinsa": [...], "japan": [...], "china": [...], "taiwan": [...], "meta": {...}}
    """
    if active_categories is None:
        active_categories = ["japan", "china", "taiwan"]

    def _s(msg):
        if on_status:
            on_status(msg)

    def _p(val, text=""):
        if on_progress:
            on_progress(min(val, 1.0), text)

    crawler  = NewsCrawler(days=days)
    collected = {cat: [] for cat in ["japan", "china", "taiwan"]}
    total_sources = sum(len(SOURCES[c]) for c in active_categories)
    done = 0

    for cat in active_categories:
        label = {"japan": "ğŸ‡¯ğŸ‡µ ì¼ë³¸", "china": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­", "taiwan": "ğŸ‡¹ğŸ‡¼ ëŒ€ë§Œ"}[cat]
        _s(f"{label} ë§¤ì²´ ìˆ˜ì§‘ ì¤‘...")

        def _cb(name, kw, _label=label):
            nonlocal done
            done += 1
            _p(done / total_sources * 0.6, f"{_label} Â· {name} ({kw})")

        collected[cat] = crawler.crawl_category(cat, keyword_ko, _cb)

    # ë¬´ì‹ ì‚¬ ê¸°ì‚¬ ë¶„ë¦¬
    musinsa     = []
    musinsa_kws = ["ë¬´ì‹ ì‚¬", "musinsa", "ãƒ ã‚·ãƒ³ã‚µ"]
    for cat in active_categories:
        flagged = [
            a for a in collected[cat]
            if any(k in a.get("title", "").lower() for k in musinsa_kws)
        ]
        musinsa.extend(flagged)
        collected[cat] = [a for a in collected[cat] if a not in flagged]

    # ë²ˆì—­
    all_t = musinsa + collected["japan"] + collected["china"] + collected["taiwan"]
    _s(f"Google ë²ˆì—­ ì²˜ë¦¬ ì¤‘ (ì´ {len(all_t)}ê±´)...")

    def _tcb(cur, total, source):
        _p(0.6 + (cur / max(total, 1)) * 0.4, f"ë²ˆì—­ ì¤‘ {cur}/{total} Â· {source}")

    translate_articles(all_t, _tcb)
    _p(1.0, "ì™„ë£Œ!")

    return {
        "musinsa": musinsa,
        "japan":   collected["japan"],
        "china":   collected["china"],
        "taiwan":  collected["taiwan"],
        "meta": {
            "keyword":      keyword_ko,
            "days":         days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M"),
        },
    }

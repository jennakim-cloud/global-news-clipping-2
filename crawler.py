"""
crawler.py - ë‰´ìŠ¤ ìˆ˜ì§‘ & Google ë²ˆì—­ ëª¨ë“ˆ
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ë¬´ì‹ ì‚¬",          "zh": "MUSINSA",   "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",   "zh": "Kç¾å¦†",     "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",              "zh": "ç”µå•†",      "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",     "zh": "æ—¶å°š",      "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",        "zh": "é›¶å”®",      "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",     "zh": "ç¾å¦†",      "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",             "zh": "SPA",       "tw": "SPA"},
    "ëŸ­ì…”ë¦¬":      {"ja": "ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼",  "zh": "å¥¢ä¾ˆå“",    "tw": "å¥¢ä¾ˆå“"},
    "ì§€ì†ê°€ëŠ¥ì„±":  {"ja": "ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£", "zh": "å¯æŒç»­å‘å±•", "tw": "æ°¸çºŒç™¼å±•"},
}

SOURCES = {
    "japan": [
        {"name": "WWD Japan",        "url": "https://www.wwdjapan.com",      "search_url": "https://www.wwdjapan.com/search?q={keyword}",              "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ"},
        {"name": "Fashionsnap",      "url": "https://www.fashionsnap.com",   "search_url": "https://www.fashionsnap.com/?s={keyword}",                  "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ"},
        {"name": "Yahoo Japan",      "url": "https://news.yahoo.co.jp",      "search_url": "https://news.yahoo.co.jp/search?p={keyword}&ei=UTF-8",      "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ"},
        {"name": "æ—¥çµŒMJ",           "url": "https://www.nikkei.com",        "search_url": "https://www.nikkei.com/search?keyword={keyword}",           "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ"},
        {"name": "ç¹Šç ”æ–°è",         "url": "https://senken.co.jp",          "search_url": "https://senken.co.jp/?s={keyword}",                         "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ"},
    ],
    "china": [
        {"name": "ç•Œé¢æ–°é—»",         "url": "https://www.jiemian.com",       "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",    "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª",             "url": "https://36kr.com",              "search_url": "https://36kr.com/search/articles/{keyword}",                "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "äº¿é‚¦åŠ¨åŠ›",         "url": "https://www.ebrun.com",         "search_url": "https://www.ebrun.com/search/?q={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD Greater China","url": "https://wwdgreaterchina.com",   "search_url": "https://wwdgreaterchina.com/?s={keyword}",                  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Vogue China",      "url": "https://www.vogue.com.cn",      "search_url": "https://www.vogue.com.cn/search?q={keyword}",               "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "ç¬¬ä¸€è´¢ç»",         "url": "https://www.yicai.com",         "search_url": "https://www.yicai.com/search/?keys={keyword}",              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "èµ¢å•†ç½‘",           "url": "https://m.winshang.com",        "search_url": "https://m.winshang.com/search.html?keyword={keyword}",      "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ–°æµª",             "url": "https://www.sina.com.cn",       "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news",  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co",          "url": "https://luxe.co",               "search_url": "https://luxe.co/?s={keyword}",                              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        {"name": "æ•¸ä½æ™‚ä»£",         "url": "https://www.bnext.com.tw",      "search_url": "https://www.bnext.com.tw/search/{keyword}",                 "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
        {"name": "å·¥å•†æ™‚å ±",         "url": "https://www.ctee.com.tw",       "search_url": "https://www.ctee.com.tw/search?q={keyword}",                "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
    ],
}

# â”€â”€â”€ ë‚ ì§œ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DATE_PATTERNS = [
    (r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", "%Y-%m-%dT%H:%M:%S"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", "%Y-%m-%d %H:%M:%S"),
    (r"\d{4}/\d{2}/\d{2} \d{2}:\d{2}",        "%Y/%m/%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}",        "%Y-%m-%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2}",                    "%Y-%m-%d"),
    (r"\d{4}/\d{2}/\d{2}",                    "%Y/%m/%d"),
    (r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",            "%Yå¹´%mæœˆ%dæ—¥"),
]

def parse_date(text: str):
    if not text:
        return None
    text = text.strip()
    # ISO íƒ€ì„ì¡´ ì œê±°
    text = re.sub(r"[+Z]\d{2}:?\d{0,2}$", "", text)
    for pattern, fmt in DATE_PATTERNS:
        m = re.search(pattern, text)
        if m:
            try:
                return datetime.strptime(m.group(0), fmt)
            except ValueError:
                continue
    return None

def clean_text(text: str) -> str:
    """HTML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    if not text:
        return ""
    # HTML íƒœê·¸ ì™„ì „ ì œê±°
    text = re.sub(r"<[^>]+>", " ", text)
    # HTML ì—”í‹°í‹° ì¹˜í™˜
    text = (text.replace("&amp;", "&").replace("&lt;", "<")
                .replace("&gt;", ">").replace("&nbsp;", " ").replace("&quot;", '"'))
    # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\s+", " ", text).strip()
    return text

# â”€â”€â”€ Google ë²ˆì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text or not text.strip():
        return text
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS,
        )
        resp.raise_for_status()
        data = resp.json()
        return "".join(seg[0] for seg in data[0] if seg[0]).strip() or text
    except Exception:
        return text

def translate_keyword_to_lang(keyword_ko: str, lang: str) -> str:
    if keyword_ko in KEYWORD_TRANSLATIONS:
        return KEYWORD_TRANSLATIONS[keyword_ko].get(lang, keyword_ko)
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": "ko", "tl": lang_map.get(lang, "ja"), "dt": "t", "q": keyword_ko},
            timeout=10, headers=HEADERS,
        )
        data = resp.json()
        return "".join(seg[0] for seg in data[0] if seg[0]).strip() or keyword_ko
    except Exception:
        return keyword_ko

# â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch(self, url: str, timeout: int = 15):
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "html.parser")
        except Exception:
            return None

    def is_within_cutoff(self, date_str: str) -> bool:
        """
        [Fix 1] ë‚ ì§œë¥¼ ëª…í™•íˆ íŒŒì‹±í•œ ê²½ìš°ì—ë§Œ í¬í•¨.
        ë‚ ì§œ ë¶ˆëª…(íŒŒì‹± ì‹¤íŒ¨) â†’ False (ê¸°ê°„ ì™¸ ê¸°ì‚¬ ì˜¤ì—¼ ë°©ì§€ ìš°ì„ ).
        """
        dt = parse_date(date_str)
        if dt is None:
            return False
        return dt >= self.cutoff

    def _find_date_in_tag(self, tag) -> str:
        """íƒœê·¸ ë‚´ì—ì„œ ë‚ ì§œ ë¬¸ìì—´ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ìˆœì°¨ ì‹œë„)"""
        # 1) <time datetime="">
        t = tag.find("time")
        if t:
            candidate = t.get("datetime") or t.get_text(strip=True)
            if parse_date(candidate):
                return candidate

        # 2) classì— date/time/pub í¬í•¨ íƒœê·¸
        d = tag.find(True, class_=re.compile(r"\b(date|time|pub|posted|created|updated)\b", re.I))
        if d:
            candidate = d.get("datetime") or d.get_text(strip=True)
            if parse_date(candidate):
                return candidate

        # 3) í…ìŠ¤íŠ¸ íŒ¨í„´ ì§ì ‘ íƒìƒ‰
        raw = tag.get_text(" ", strip=True)
        for pattern, _ in DATE_PATTERNS:
            m = re.search(pattern, raw)
            if m:
                return m.group(0)

        return ""

    def parse_generic(self, soup: BeautifulSoup, base_url: str) -> list[dict]:
        candidates = []
        seen = set()

        # (A) <article> ê¸°ë°˜
        for article in soup.find_all("article"):
            title_tag = article.find(["h1", "h2", "h3", "h4"])
            a_tag = article.find("a", href=True)
            if not title_tag or not a_tag:
                continue
            title = clean_text(title_tag.get_text())
            url   = urljoin(base_url, a_tag["href"])
            date  = self._find_date_in_tag(article)
            if url not in seen and len(title) > 5:
                seen.add(url)
                candidates.append({"title": title, "url": url, "date": date})

        # (B) h2/h3 ê¸°ë°˜ (article ì—†ì„ ë•Œ)
        if not candidates:
            for h in soup.find_all(["h2", "h3"]):
                a_tag = h.find("a", href=True) or h.find_parent("a", href=True)
                if not a_tag:
                    continue
                title  = clean_text(h.get_text())
                url    = urljoin(base_url, a_tag["href"])
                parent = h.find_parent(["li", "div", "section"])
                date   = self._find_date_in_tag(parent) if parent else ""
                if url not in seen and len(title) > 5:
                    seen.add(url)
                    candidates.append({"title": title, "url": url, "date": date})

        # [Fix 1] ì—„ê²©í•œ ë‚ ì§œ í•„í„° ì ìš©
        return [c for c in candidates if self.is_within_cutoff(c["date"])][:12]

    def search_source(self, source: dict, keyword: str) -> list[dict]:
        soup = self.fetch(source["search_url"].format(keyword=quote(keyword)))
        if not soup:
            return []
        results = self.parse_generic(soup, source["url"])
        for r in results:
            r.update({"source": source["name"], "source_url": source["url"],
                       "language": source["language"], "flag": source.get("flag", "")})
        return results

    def crawl_category(self, category: str, keyword_ko: str, progress_callback=None) -> list[dict]:
        all_articles = []
        for source in SOURCES.get(category, []):
            keyword = translate_keyword_to_lang(keyword_ko, source["language"])
            if progress_callback:
                progress_callback(source["name"], keyword)
            all_articles.extend(self.search_source(source, keyword))
            time.sleep(0.8)
        return all_articles

# â”€â”€â”€ ë²ˆì—­ ì¼ê´„ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_articles(articles: list[dict], progress_callback=None) -> list[dict]:
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    for i, a in enumerate(articles):
        if progress_callback:
            progress_callback(i + 1, len(articles), a.get("source", ""))
        src = lang_map.get(a.get("language", "ja"), "auto")
        a["title_ko"] = translate_to_korean(a["title"], src_lang=src)
        time.sleep(0.3)
    return articles

# â”€â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    keyword_ko: str,
    days: int = 7,
    active_categories: list = None,
    on_status=None,
    on_progress=None,
) -> dict:
    if active_categories is None:
        active_categories = ["japan", "china", "taiwan"]

    def _s(msg):
        if on_status: on_status(msg)
    def _p(val, text=""):
        if on_progress: on_progress(min(val, 1.0), text)

    crawler = NewsCrawler(days=days)
    collected = {cat: [] for cat in ["japan", "china", "taiwan"]}
    total_sources = sum(len(SOURCES[c]) for c in active_categories)
    done = 0

    for cat in active_categories:
        label = {"japan": "ğŸ‡¯ğŸ‡µ ì¼ë³¸", "china": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­", "taiwan": "ğŸ‡¹ğŸ‡¼ ëŒ€ë§Œ"}[cat]
        _s(f"{label} ë§¤ì²´ ìˆ˜ì§‘ ì¤‘...")

        def _cb(name, kw, _label=label):
            nonlocal done
            done += 1
            _p(done / total_sources * 0.6, f"{_label} Â· {name} ({kw})")

        collected[cat] = crawler.crawl_category(cat, keyword_ko, _cb)

    # ë¬´ì‹ ì‚¬ ë¶„ë¦¬
    musinsa = []
    musinsa_kws = ["ë¬´ì‹ ì‚¬", "musinsa", "ãƒ ã‚·ãƒ³ã‚µ"]
    for cat in active_categories:
        flagged = [a for a in collected[cat] if any(k in a.get("title","").lower() for k in musinsa_kws)]
        musinsa.extend(flagged)
        collected[cat] = [a for a in collected[cat] if a not in flagged]

    # ë²ˆì—­
    all_t = musinsa + collected["japan"] + collected["china"] + collected["taiwan"]
    _s(f"Google ë²ˆì—­ ì²˜ë¦¬ ì¤‘ (ì´ {len(all_t)}ê±´)...")

    def _tcb(cur, total, source):
        _p(0.6 + (cur / max(total, 1)) * 0.4, f"ë²ˆì—­ ì¤‘ {cur}/{total} Â· {source}")

    translate_articles(all_t, _tcb)
    _p(1.0, "ì™„ë£Œ!")

    return {
        "musinsa": musinsa,
        "japan":   collected["japan"],
        "china":   collected["china"],
        "taiwan":  collected["taiwan"],
        "meta": {
            "keyword":      keyword_ko,
            "days":         days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M"),
        },
    }

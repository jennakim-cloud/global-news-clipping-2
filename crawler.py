"""
crawler.py - ë‰´ìŠ¤ ìˆ˜ì§‘ & Google ë²ˆì—­ ëª¨ë“ˆ
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í—¤ë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ë¬´ì‹ ì‚¬",          "zh": "MUSINSA",   "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",   "zh": "Kç¾å¦†",     "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",              "zh": "ç”µå•†",      "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",     "zh": "æ—¶å°š",      "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",        "zh": "é›¶å”®",      "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",     "zh": "ç¾å¦†",      "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",             "zh": "SPA",       "tw": "SPA"},
    "ëŸ­ì…”ë¦¬":      {"ja": "ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼",  "zh": "å¥¢ä¾ˆå“",    "tw": "å¥¢ä¾ˆå“"},
    "ì§€ì†ê°€ëŠ¥ì„±":  {"ja": "ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£", "zh": "å¯æŒç»­å‘å±•", "tw": "æ°¸çºŒç™¼å±•"},
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë§¤ì²´ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    "japan": [
        {
            "name": "WWD Japan",
            "url": "https://www.wwdjapan.com",
            "search_url": "https://www.wwdjapan.com/search?q={keyword}",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
        },
        {
            "name": "Fashionsnap",
            "url": "https://www.fashionsnap.com",
            "search_url": "https://www.fashionsnap.com/?s={keyword}",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
        },
        {
            "name": "Yahoo Japan ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "url": "https://news.yahoo.co.jp",
            "search_url": "https://news.yahoo.co.jp/search?p={keyword}&ei=UTF-8",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
        },
        {
            "name": "æ—¥çµŒMJ",
            "url": "https://www.nikkei.com",
            "search_url": "https://www.nikkei.com/search?keyword={keyword}",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
        },
        {
            "name": "ç¹Šç ”æ–°è",
            "url": "https://senken.co.jp",
            "search_url": "https://senken.co.jp/?s={keyword}",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
        },
    ],
    "china": [
        {
            "name": "ç•Œé¢æ–°é—»",
            "url": "https://www.jiemian.com",
            "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "36æ°ª",
            "url": "https://36kr.com",
            "search_url": "https://36kr.com/search/articles/{keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "äº¿é‚¦åŠ¨åŠ›",
            "url": "https://www.ebrun.com",
            "search_url": "https://www.ebrun.com/search/?q={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "WWD Greater China",
            "url": "https://wwdgreaterchina.com",
            "search_url": "https://wwdgreaterchina.com/?s={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "Vogue China",
            "url": "https://www.vogue.com.cn",
            "search_url": "https://www.vogue.com.cn/search?q={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "ç¬¬ä¸€è´¢ç»",
            "url": "https://www.yicai.com",
            "search_url": "https://www.yicai.com/search/?keys={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "èµ¢å•†ç½‘",
            "url": "https://m.winshang.com",
            "search_url": "https://m.winshang.com/search.html?keyword={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "æ–°æµª",
            "url": "https://www.sina.com.cn",
            "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
        {
            "name": "Luxe.co",
            "url": "https://luxe.co",
            "search_url": "https://luxe.co/?s={keyword}",
            "language": "zh",
            "flag": "ğŸ‡¨ğŸ‡³",
        },
    ],
    "taiwan": [
        {
            "name": "æ•¸ä½æ™‚ä»£",
            "url": "https://www.bnext.com.tw",
            "search_url": "https://www.bnext.com.tw/search/{keyword}",
            "language": "tw",
            "flag": "ğŸ‡¹ğŸ‡¼",
        },
        {
            "name": "å·¥å•†æ™‚å ±",
            "url": "https://www.ctee.com.tw",
            "search_url": "https://www.ctee.com.tw/search?q={keyword}",
            "language": "tw",
            "flag": "ğŸ‡¹ğŸ‡¼",
        },
    ],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Google ë²ˆì—­ (ë¹„ê³µì‹ ë¬´ë£Œ API)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    """
    Google ë²ˆì—­ ë¹„ê³µì‹ APIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ë²ˆì—­.
    ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜.
    """
    if not text or not text.strip():
        return text
    try:
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            "client": "gtx",
            "sl": src_lang,
            "tl": "ko",
            "dt": "t",
            "q": text,
        }
        resp = requests.get(url, params=params, timeout=10, headers=HEADERS)
        resp.raise_for_status()
        data = resp.json()
        # ë²ˆì—­ ê²°ê³¼ ì¡°ê°ë“¤ì„ í•©ì¹¨
        translated = "".join(seg[0] for seg in data[0] if seg[0])
        return translated.strip() or text
    except Exception:
        return text


def translate_keyword_to_lang(keyword_ko: str, lang: str) -> str:
    """í‚¤ì›Œë“œë¥¼ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­ (ì‚¬ì „ â†’ Google ë²ˆì—­ ìˆœ)"""
    # ì‚¬ì „ì— ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if keyword_ko in KEYWORD_TRANSLATIONS:
        return KEYWORD_TRANSLATIONS[keyword_ko].get(lang, keyword_ko)
    # ì—†ìœ¼ë©´ Google ë²ˆì—­ ì‚¬ìš©
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    tl = lang_map.get(lang, "ja")
    url = "https://translate.googleapis.com/translate_a/single"
    params = {"client": "gtx", "sl": "ko", "tl": tl, "dt": "t", "q": keyword_ko}
    try:
        resp = requests.get(url, params=params, timeout=10, headers=HEADERS)
        data = resp.json()
        return "".join(seg[0] for seg in data[0] if seg[0]).strip() or keyword_ko
    except Exception:
        return keyword_ko


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ëŸ¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch(self, url: str, timeout: int = 15):
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "html.parser")
        except Exception as e:
            return None, str(e)

    def _parse_date(self, text: str):
        if not text:
            return None
        text = text.strip()
        patterns = [
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%dT%H:%M:%SZ",
            "%Y-%m-%dT%H:%M:%S+09:00",
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M",
            "%Y-%m-%d",
            "%Y/%m/%d",
            "%Yå¹´%mæœˆ%dæ—¥",
        ]
        for fmt in patterns:
            try:
                dt = datetime.strptime(text[:len(fmt) + 2], fmt)
                if dt.year < 2000:
                    dt = dt.replace(year=datetime.now().year)
                return dt
            except ValueError:
                continue
        return None

    def is_recent(self, date_str: str) -> bool:
        dt = self._parse_date(date_str)
        if dt is None:
            return True
        return dt >= self.cutoff

    def parse_generic(self, soup, base_url: str) -> list[dict]:
        articles = []
        seen = set()
        candidates = []

        for tag in soup.find_all("article"):
            a = tag.find("a", href=True)
            title_tag = tag.find(["h1", "h2", "h3", "h4"])
            date_tag = tag.find(["time", "span"], class_=re.compile(r"date|time|pub", re.I))
            if a and title_tag:
                candidates.append({
                    "title": title_tag.get_text(strip=True),
                    "url": urljoin(base_url, a["href"]),
                    "date": (date_tag.get("datetime") or date_tag.get_text(strip=True)) if date_tag else "",
                })

        if not candidates:
            for tag in soup.find_all(["h2", "h3"]):
                a = tag.find("a", href=True) or tag.find_parent("a", href=True)
                if a:
                    candidates.append({
                        "title": tag.get_text(strip=True),
                        "url": urljoin(base_url, a["href"]),
                        "date": "",
                    })

        for c in candidates:
            if c["url"] in seen:
                continue
            seen.add(c["url"])
            if self.is_recent(c["date"]) and len(c["title"]) > 5:
                articles.append(c)

        return articles[:12]

    def search_source(self, source: dict, keyword: str) -> list[dict]:
        kw_encoded = quote(keyword)
        search_url = source["search_url"].format(keyword=kw_encoded)
        soup = self.fetch(search_url)
        if not soup or isinstance(soup, tuple):
            return []
        results = self.parse_generic(soup, source["url"])
        for r in results:
            r["source"] = source["name"]
            r["source_url"] = source["url"]
            r["language"] = source["language"]
            r["flag"] = source.get("flag", "")
        return results

    def crawl_category(
        self,
        category: str,
        keyword_ko: str,
        progress_callback=None,
    ) -> list[dict]:
        sources = SOURCES.get(category, [])
        all_articles = []

        for i, source in enumerate(sources):
            lang = source["language"]
            keyword = translate_keyword_to_lang(keyword_ko, lang)

            if progress_callback:
                progress_callback(source["name"], keyword)

            articles = self.search_source(source, keyword)
            all_articles.extend(articles)
            time.sleep(0.8)

        return all_articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë²ˆì—­ ì¼ê´„ ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_articles(
    articles: list[dict],
    progress_callback=None,
) -> list[dict]:
    """ê¸°ì‚¬ ì œëª©ì„ í•œêµ­ì–´ë¡œ ì¼ê´„ ë²ˆì—­"""
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    for i, a in enumerate(articles):
        if progress_callback:
            progress_callback(i + 1, len(articles), a.get("source", ""))
        src = lang_map.get(a.get("language", "ja"), "auto")
        a["title_ko"] = translate_to_korean(a["title"], src_lang=src)
        time.sleep(0.3)  # Google ë²ˆì—­ rate limit ë°©ì§€
    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì²´ íŒŒì´í”„ë¼ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    keyword_ko: str,
    days: int = 7,
    on_status=None,       # ìƒíƒœ ë©”ì‹œì§€ ì½œë°±: fn(msg: str)
    on_progress=None,     # ì§„í–‰ë¥  ì½œë°±: fn(value: float, text: str)
) -> dict:
    """
    ìˆ˜ì§‘ â†’ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.
    ë°˜í™˜: {
        "musinsa": [...],
        "japan": [...],
        "china": [...],
        "taiwan": [...],
        "meta": {"keyword": ..., "days": ..., "generated_at": ...}
    }
    """
    def _status(msg):
        if on_status:
            on_status(msg)

    def _prog(val, text=""):
        if on_progress:
            on_progress(val, text)

    crawler = NewsCrawler(days=days)
    all_collected = {"japan": [], "china": [], "taiwan": []}
    total_sources = sum(len(SOURCES[c]) for c in ["japan", "china", "taiwan"])
    done = 0

    for category in ["japan", "china", "taiwan"]:
        cat_label = {"japan": "ğŸ‡¯ğŸ‡µ ì¼ë³¸", "china": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­", "taiwan": "ğŸ‡¹ğŸ‡¼ ëŒ€ë§Œ"}[category]
        _status(f"{cat_label} ë§¤ì²´ ìˆ˜ì§‘ ì¤‘...")

        def _prog_cb(name, kw, _cat=category, _label=cat_label):
            nonlocal done
            done += 1
            _prog(done / total_sources * 0.6, f"{_label} Â· {name} ({kw})")

        articles = crawler.crawl_category(category, keyword_ko, progress_callback=_prog_cb)
        all_collected[category] = articles

    # ë¬´ì‹ ì‚¬ í•„í„°ë§
    musinsa_kws = {"ë¬´ì‹ ì‚¬", "musinsa", "ãƒ ã‚·ãƒ³ã‚µ"}
    musinsa_articles = []
    for cat in ["japan", "china", "taiwan"]:
        flagged = [
            a for a in all_collected[cat]
            if any(k in a.get("title", "").lower() for k in musinsa_kws)
        ]
        musinsa_articles.extend(flagged)
        all_collected[cat] = [a for a in all_collected[cat] if a not in flagged]

    # ì „ì²´ ë²ˆì—­
    all_to_translate = (
        musinsa_articles
        + all_collected["japan"]
        + all_collected["china"]
        + all_collected["taiwan"]
    )
    total_translate = len(all_to_translate)
    _status(f"Google ë²ˆì—­ ì²˜ë¦¬ ì¤‘ (ì´ {total_translate}ê±´)...")

    def _trans_cb(cur, total, source):
        _prog(0.6 + (cur / total) * 0.4, f"ë²ˆì—­ ì¤‘ {cur}/{total} Â· {source}")

    translate_articles(all_to_translate, progress_callback=_trans_cb)
    _prog(1.0, "ì™„ë£Œ!")

    return {
        "musinsa": musinsa_articles,
        "japan": all_collected["japan"],
        "china": all_collected["china"],
        "taiwan": all_collected["taiwan"],
        "meta": {
            "keyword": keyword_ko,
            "days": days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M"),
        },
    }

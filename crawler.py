"""
crawler.py - ë‰´ìŠ¤ ìˆ˜ì§‘ & Google ë²ˆì—­ ëª¨ë“ˆ

[ì¼ë³¸] Google News RSS ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰
  - JS ë Œë”ë§ ì—†ì´ XML íŒŒì‹±ìœ¼ë¡œ ì•ˆì •ì  ìˆ˜ì§‘
  - API í‚¤ ë¶ˆí•„ìš”
  - URL: https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja

[ì¤‘êµ­/ëŒ€ë§Œ] ë§¤ì²´ë³„ HTML í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re
import xml.etree.ElementTree as ET

# â”€â”€â”€ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€ í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",        "zh": "MUSINSA",    "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",   "zh": "Kç¾å¦†",      "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",              "zh": "ç”µå•†",       "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",     "zh": "æ—¶å°š",       "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",        "zh": "é›¶å”®",       "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",     "zh": "ç¾å¦†",       "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",             "zh": "SPA",        "tw": "SPA"},
    "ëŸ­ì…”ë¦¬":      {"ja": "ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼",  "zh": "å¥¢ä¾ˆå“",     "tw": "å¥¢ä¾ˆå“"},
    "ì§€ì†ê°€ëŠ¥ì„±":  {"ja": "ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£", "zh": "å¯æŒç»­å‘å±•", "tw": "æ°¸çºŒç™¼å±•"},
}

# â”€â”€â”€ ë§¤ì²´ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    "japan": [
        {
            "name": "Google News (æ—¥æœ¬)",
            "url": "https://news.google.com",
            # hl=ja : ì¼ë³¸ì–´ UI / gl=JP : ì¼ë³¸ ì§€ì—­ / ceid=JP:ja : ì¼ë³¸ì–´ ì—ë””ì…˜
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
            "parser": "google_news_rss",
        },
    ],
    "china": [
        {"name": "ç•Œé¢æ–°é—»",          "url": "https://www.jiemian.com",     "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",   "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª",              "url": "https://36kr.com",            "search_url": "https://36kr.com/search/articles/{keyword}",               "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "äº¿é‚¦åŠ¨åŠ›",          "url": "https://www.ebrun.com",       "search_url": "https://www.ebrun.com/search/?q={keyword}",                "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD Greater China", "url": "https://wwdgreaterchina.com", "search_url": "https://wwdgreaterchina.com/?s={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Vogue China",       "url": "https://www.vogue.com.cn",    "search_url": "https://www.vogue.com.cn/search?q={keyword}",              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "ç¬¬ä¸€è´¢ç»",          "url": "https://www.yicai.com",       "search_url": "https://www.yicai.com/search/?keys={keyword}",             "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "èµ¢å•†ç½‘",            "url": "https://m.winshang.com",      "search_url": "https://m.winshang.com/search.html?keyword={keyword}",     "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ–°æµª",              "url": "https://www.sina.com.cn",     "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co",           "url": "https://luxe.co",             "search_url": "https://luxe.co/?s={keyword}",                             "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        {"name": "æ•¸ä½æ™‚ä»£", "url": "https://www.bnext.com.tw", "search_url": "https://www.bnext.com.tw/search/{keyword}",  "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
        {"name": "å·¥å•†æ™‚å ±", "url": "https://www.ctee.com.tw",  "search_url": "https://www.ctee.com.tw/search?q={keyword}", "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼"},
    ],
}

# â”€â”€â”€ ë‚ ì§œ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RSS pubDate í˜•ì‹ (RFC 2822)
RFC2822_PATTERN = re.compile(
    r"(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})\s+(\d{2}:\d{2}:\d{2})"
)
MONTH_MAP = {
    "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4,  "May": 5,  "Jun": 6,
    "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12,
}

DATE_PATTERNS = [
    (r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", "%Y-%m-%dT%H:%M:%S"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", "%Y-%m-%d %H:%M:%S"),
    (r"\d{4}/\d{2}/\d{2} \d{2}:\d{2}",        "%Y/%m/%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}",        "%Y-%m-%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2}",                    "%Y-%m-%d"),
    (r"\d{4}/\d{2}/\d{2}",                    "%Y/%m/%d"),
    (r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",            "%Yå¹´%mæœˆ%dæ—¥"),
]


def parse_date(text: str):
    """
    ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±.
    RSS pubDate (RFC 2822) / ISO 8601 / ì¼ë°˜ ë‚ ì§œ í˜•ì‹ ëª¨ë‘ ì§€ì›.
    """
    if not text:
        return None
    text = text.strip()

    # RFC 2822: "20 Feb 2026 03:00:00 GMT"
    m = RFC2822_PATTERN.search(text)
    if m:
        day, mon_str, year, time_str = m.groups()
        try:
            return datetime(int(year), MONTH_MAP[mon_str], int(day),
                            *map(int, time_str.split(":")))
        except (ValueError, KeyError):
            pass

    # ISO íƒ€ì„ì¡´ ì œê±° í›„ ì¼ë°˜ íŒ¨í„´
    text_clean = re.sub(r"[+Z]\d{2}:?\d{0,2}$", "", text)
    for pattern, fmt in DATE_PATTERNS:
        m = re.search(pattern, text_clean)
        if m:
            try:
                return datetime.strptime(m.group(0), fmt)
            except ValueError:
                continue

    return None


def clean_text(text: str) -> str:
    """HTML íƒœê·¸Â·ì—”í‹°í‹°Â·ê³¼ë„í•œ ê³µë°± ì œê±°"""
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = (text.replace("&amp;", "&").replace("&lt;", "<")
                .replace("&gt;", ">").replace("&nbsp;", " ").replace("&quot;", '"'))
    return re.sub(r"\s+", " ", text).strip()


# â”€â”€â”€ Google ë²ˆì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text or not text.strip():
        return text
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS,
        )
        resp.raise_for_status()
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or text
    except Exception:
        return text


def translate_keyword_to_lang(keyword_ko: str, lang: str) -> str:
    """í‚¤ì›Œë“œë¥¼ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­ (ì‚¬ì „ ìš°ì„  â†’ Google ë²ˆì—­ fallback)"""
    if keyword_ko in KEYWORD_TRANSLATIONS:
        return KEYWORD_TRANSLATIONS[keyword_ko].get(lang, keyword_ko)
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": "ko", "tl": lang_map.get(lang, "ja"), "dt": "t", "q": keyword_ko},
            timeout=10, headers=HEADERS,
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or keyword_ko
    except Exception:
        return keyword_ko


# â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch_raw(self, url: str, timeout: int = 15) -> str | None:
        """ì›ì‹œ í…ìŠ¤íŠ¸ ë°˜í™˜ (RSS XML íŒŒì‹±ìš©)"""
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return resp.text
        except Exception:
            return None

    def fetch(self, url: str, timeout: int = 15) -> BeautifulSoup | None:
        """BeautifulSoup ë°˜í™˜ (HTML í¬ë¡¤ë§ìš©)"""
        raw = self.fetch_raw(url, timeout)
        return BeautifulSoup(raw, "html.parser") if raw else None

    def is_within_cutoff(self, date_str: str) -> bool:
        """ë‚ ì§œë¥¼ ëª…í™•íˆ íŒŒì‹±í•œ ê²½ìš°ì—ë§Œ í¬í•¨ (ë‚ ì§œ ë¶ˆëª… â†’ False)"""
        dt = parse_date(date_str)
        if dt is None:
            return False
        return dt >= self.cutoff

    # â”€â”€ Google News RSS íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_google_news_rss(self, raw_xml: str, source_flag: str = "ğŸ‡¯ğŸ‡µ") -> list[dict]:
        """
        Google News RSS XMLì„ íŒŒì‹±í•´ ê¸°ì‚¬ ëª©ë¡ ë°˜í™˜.

        RSS <item> êµ¬ì¡°:
          <title>ê¸°ì‚¬ ì œëª© - ë§¤ì²´ëª…</title>
          <link>https://news.google.com/rss/articles/...</link>
          <pubDate>Thu, 20 Feb 2026 03:00:00 GMT</pubDate>
          <source url="...">ë§¤ì²´ëª…</source>
        """
        results = []
        try:
            root = ET.fromstring(raw_xml.encode("utf-8"))
        except ET.ParseError:
            return []

        # XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬
        ns = {"media": "http://search.yahoo.com/mrss/"}

        for item in root.iter("item"):
            # ì œëª© (Google News RSSëŠ” "ê¸°ì‚¬ì œëª© - ë§¤ì²´ëª…" í˜•ì‹)
            title_el = item.find("title")
            if title_el is None or not title_el.text:
                continue
            raw_title = clean_text(title_el.text)

            # "ì œëª© - ë§¤ì²´ëª…" ì—ì„œ ë§¤ì²´ëª… ë¶„ë¦¬
            media_name = ""
            if " - " in raw_title:
                parts      = raw_title.rsplit(" - ", 1)
                title      = parts[0].strip()
                media_name = parts[1].strip()
            else:
                title = raw_title

            if len(title) < 5:
                continue

            # URL
            link_el = item.find("link")
            # Google News RSSì˜ <link>ëŠ” í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ë‹¤ìŒ ìš”ì†Œ ì•ì— ìœ„ì¹˜
            # ëŒ€ì•ˆ: <guid>
            url = ""
            if link_el is not None and link_el.text:
                url = link_el.text.strip()
            if not url:
                guid_el = item.find("guid")
                if guid_el is not None and guid_el.text:
                    url = guid_el.text.strip()

            # <source> íƒœê·¸ì—ì„œ ë§¤ì²´ëª… ë³´ì™„
            source_el = item.find("source")
            if source_el is not None and source_el.text:
                media_name = media_name or clean_text(source_el.text)

            # ë‚ ì§œ
            pub_el = item.find("pubDate")
            date_str = pub_el.text.strip() if pub_el is not None and pub_el.text else ""

            if not self.is_within_cutoff(date_str):
                continue

            results.append({
                "title":   title,
                "url":     url,
                "date":    date_str,
                "media":   media_name,   # ì›ë³¸ ë§¤ì²´ëª… ë³´ì¡´
            })

        return results[:20]

    # â”€â”€ ë²”ìš© HTML íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _find_date_in_tag(self, tag) -> str:
        t = tag.find("time")
        if t:
            candidate = t.get("datetime") or t.get_text(strip=True)
            if parse_date(candidate):
                return candidate
        d = tag.find(True, class_=re.compile(r"\b(date|time|pub|posted|created|updated)\b", re.I))
        if d:
            candidate = d.get("datetime") or d.get_text(strip=True)
            if parse_date(candidate):
                return candidate
        raw = tag.get_text(" ", strip=True)
        for pattern, _ in DATE_PATTERNS:
            m = re.search(pattern, raw)
            if m:
                return m.group(0)
        return ""

    def parse_generic(self, soup: BeautifulSoup, base_url: str) -> list[dict]:
        candidates = []
        seen = set()

        for article in soup.find_all("article"):
            title_tag = article.find(["h1", "h2", "h3", "h4"])
            a_tag     = article.find("a", href=True)
            if not title_tag or not a_tag:
                continue
            title = clean_text(title_tag.get_text())
            url   = urljoin(base_url, a_tag["href"])
            date  = self._find_date_in_tag(article)
            if url not in seen and len(title) > 5:
                seen.add(url)
                candidates.append({"title": title, "url": url, "date": date})

        if not candidates:
            for h in soup.find_all(["h2", "h3"]):
                a_tag = h.find("a", href=True) or h.find_parent("a", href=True)
                if not a_tag:
                    continue
                title  = clean_text(h.get_text())
                url    = urljoin(base_url, a_tag["href"])
                parent = h.find_parent(["li", "div", "section"])
                date   = self._find_date_in_tag(parent) if parent else ""
                if url not in seen and len(title) > 5:
                    seen.add(url)
                    candidates.append({"title": title, "url": url, "date": date})

        return [c for c in candidates if self.is_within_cutoff(c["date"])][:12]

    # â”€â”€ ë§¤ì²´ ê²€ìƒ‰ (íŒŒì„œ ë¶„ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def search_source(self, source: dict, keyword: str) -> list[dict]:
        search_url  = source["search_url"].format(keyword=quote(keyword))
        parser_name = source.get("parser", "generic")

        if parser_name == "google_news_rss":
            # RSSëŠ” raw XMLë¡œ ê°€ì ¸ì™€ì„œ ETë¡œ íŒŒì‹±
            raw = self.fetch_raw(search_url)
            results = self.parse_google_news_rss(raw, source.get("flag", "ğŸ‡¯ğŸ‡µ")) if raw else []
        else:
            soup    = self.fetch(search_url)
            results = self.parse_generic(soup, source["url"]) if soup else []

        # URL íŒ¨í„´ í•„í„° (ì¹´í…Œê³ ë¦¬Â·íƒœê·¸ ë§í¬ ì œê±°)
        exclude_patterns   = source.get("exclude_url_patterns", [])
        exclude_exact_urls = set(source.get("exclude_exact_urls", []))
        if exclude_patterns or exclude_exact_urls:
            before  = len(results)
            results = [
                r for r in results
                if r.get("url", "") not in exclude_exact_urls
                and not any(p in r.get("url", "") for p in exclude_patterns)
            ]
            if before - len(results):
                print(f"  [{source['name']}] ë¹„ê¸°ì‚¬ ë§í¬ {before - len(results)}ê±´ ì œì™¸")

        # ê³µí†µ ë©”íƒ€ ì„¸íŒ…
        for r in results:
            r.setdefault("source",     source["name"])
            r.setdefault("source_url", source["url"])
            r.setdefault("language",   source["language"])
            r.setdefault("flag",       source.get("flag", ""))

        return results

    def crawl_category(self, category: str, keyword_ko: str, progress_callback=None) -> list[dict]:
        all_articles = []
        for source in SOURCES.get(category, []):
            keyword = translate_keyword_to_lang(keyword_ko, source["language"])
            if progress_callback:
                progress_callback(source["name"], keyword)
            all_articles.extend(self.search_source(source, keyword))
            time.sleep(0.8)
        return all_articles


# â”€â”€â”€ ë²ˆì—­ ì¼ê´„ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_articles(articles: list[dict], progress_callback=None) -> list[dict]:
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    for i, a in enumerate(articles):
        if progress_callback:
            progress_callback(i + 1, len(articles), a.get("source", ""))
        src = lang_map.get(a.get("language", "ja"), "auto")
        a["title_ko"] = translate_to_korean(a["title"], src_lang=src)
        time.sleep(0.3)
    return articles


# â”€â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    keyword_ko: str,
    days: int = 7,
    active_categories: list = None,
    on_status=None,
    on_progress=None,
) -> dict:
    if active_categories is None:
        active_categories = ["japan", "china", "taiwan"]

    def _s(msg):
        if on_status: on_status(msg)

    def _p(val, text=""):
        if on_progress: on_progress(min(val, 1.0), text)

    crawler       = NewsCrawler(days=days)
    collected     = {cat: [] for cat in ["japan", "china", "taiwan"]}
    total_sources = sum(len(SOURCES[c]) for c in active_categories)
    done          = 0

    for cat in active_categories:
        label = {"japan": "ğŸ‡¯ğŸ‡µ ì¼ë³¸", "china": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­", "taiwan": "ğŸ‡¹ğŸ‡¼ ëŒ€ë§Œ"}[cat]
        _s(f"{label} ë§¤ì²´ ìˆ˜ì§‘ ì¤‘...")

        def _cb(name, kw, _label=label):
            nonlocal done
            done += 1
            _p(done / total_sources * 0.6, f"{_label} Â· {name} ({kw})")

        collected[cat] = crawler.crawl_category(cat, keyword_ko, _cb)

    # ë¬´ì‹ ì‚¬ ê¸°ì‚¬ ë¶„ë¦¬
    musinsa     = []
    musinsa_kws = ["ë¬´ì‹ ì‚¬", "musinsa", "ãƒ ã‚·ãƒ³ã‚µ"]
    for cat in active_categories:
        flagged = [
            a for a in collected[cat]
            if any(k in a.get("title", "").lower() for k in musinsa_kws)
        ]
        musinsa.extend(flagged)
        collected[cat] = [a for a in collected[cat] if a not in flagged]

    # ë²ˆì—­
    all_t = musinsa + collected["japan"] + collected["china"] + collected["taiwan"]
    _s(f"Google ë²ˆì—­ ì²˜ë¦¬ ì¤‘ (ì´ {len(all_t)}ê±´)...")

    def _tcb(cur, total, source):
        _p(0.6 + (cur / max(total, 1)) * 0.4, f"ë²ˆì—­ ì¤‘ {cur}/{total} Â· {source}")

    translate_articles(all_t, _tcb)
    _p(1.0, "ì™„ë£Œ!")

    return {
        "musinsa": musinsa,
        "japan":   collected["japan"],
        "china":   collected["china"],
        "taiwan":  collected["taiwan"],
        "meta": {
            "keyword":      keyword_ko,
            "days":         days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M"),
        },
    }

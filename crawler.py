import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re
import xml.etree.ElementTree as ET

# â”€â”€â”€ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€ í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ (ì—…ë°ì´íŠ¸ ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",              "zh": "MUSINSA",    "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",       "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",          "zh": "Kç¾å¦†",      "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",                    "zh": "ç”µå•†",       "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",            "zh": "æ—¶å°š",       "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",               "zh": "é›¶å”®",       "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",            "zh": "ç¾å¦",       "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",                   "zh": "SPA",        "tw": "SPA"},
    "íŒ¨ì…˜ ë¸Œëœë“œ": {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰",    "zh": "æ—¶å°šå“ç‰Œ",   "tw": "æ™‚å°šå“ç‰Œ"},
    "ìœ ë‹ˆí´ë¡œ":    {"ja": "ãƒ¦ãƒ‹ã‚¯ãƒ­",               "zh": "ä¼˜è¡£åº“",     "tw": "Uniqlo"},
    "ë¬´ì¸ì–‘í’ˆ":    {"ja": "ç„¡å°è‰¯å“",               "zh": "æ— å°è‰¯å“",   "tw": "ç„¡å°è‰¯å“"},
    "ì•ˆíƒ€":        {"ja": "å®‰è¸",                   "zh": "å®‰è¸",       "tw": "å®‰è¸"},
}

# â”€â”€â”€ ë§¤ì²´ ì„¤ì • (ì¤‘êµ­ & ëŒ€ë§Œ í†µí•©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCES = {
    "china": [
        # ê²€ìƒ‰ ì—”ì§„: ë°”ì´ë‘
        {
            "name": "ç™¾åº¦æ–°é—»",
            "url": "https://news.baidu.com",
            "search_url": "https://news.baidu.com/ns?word={keyword}&tn=news&from=news&ie=utf-8&rn=20",
            "language": "zh", "flag": "ğŸ‡¨ğŸ‡³", "parser": "baidu_news",
        },
        # ê°œë³„ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸
        {"name": "ç•Œé¢æ–°é—»",          "url": "https://www.jiemian.com",     "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",    "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª",              "url": "https://36kr.com",            "search_url": "https://36kr.com/search/articles/{keyword}",                "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "äº¿é‚¦åŠ¨åŠ›",          "url": "https://www.ebrun.com",       "search_url": "https://www.ebrun.com/search/?q={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ–°æµª",              "url": "https://www.sina.com.cn",     "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news",  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co",           "url": "https://luxe.co",             "search_url": "https://luxe.co/?s={keyword}",                              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD Greater China", "url": "https://wwdgreaterchina.com", "search_url": "https://wwdgreaterchina.com/?s={keyword}",                  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Vogue China",       "url": "https://www.vogue.com.cn",    "search_url": "https://www.vogue.com.cn/search?q={keyword}",               "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        # ëŒ€ë§Œ êµ¬ê¸€ ë‰´ìŠ¤ RSS
        {
            "name": "Google News (å°ç£)",
            "url": "https://news.google.com",
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
            "language": "tw",
            "flag": "ğŸ‡¹ğŸ‡¼",
            "parser": "google_news_rss",
        },
    ],
}

# â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë‚ ì§œ, í…ìŠ¤íŠ¸, ë²ˆì—­) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATE_PATTERNS = [
    (r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", "%Y-%m-%dT%H:%M:%S"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", "%Y-%m-%d %H:%M:%S"),
    (r"\d{4}/\d{2}/\d{2}", "%Y/%m/%d"),
    (r"\d{4}-\d{2}-\d{2}", "%Y-%m-%d"),
    (r"\d{4}å¹´\d{1,2}ì›”\d{1,2}ì¼", "%Yå¹´%mì›”%dì¼"),
]

def parse_date(text: str):
    if not text: return None
    text = text.strip()
    for pattern, fmt in DATE_PATTERNS:
        m = re.search(pattern, text)
        if m:
            try: return datetime.strptime(m.group(0), fmt)
            except: continue
    return None

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r"<[^>]+>", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0])
    except:
        return text

# â”€â”€â”€ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class IntegratedNewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def is_within_cutoff(self, date_str: str, is_china: bool = False) -> bool:
        if not date_str: return False
        if is_china:
            now = datetime.now()
            if "å°æ—¶å‰" in date_str:
                h = int(re.search(r"(\d+)", date_str).group(1))
                return (now - timedelta(hours=h)) >= self.cutoff
            if "åˆ†é’Ÿå‰" in date_str or "åˆšåˆš" in date_str or "ä»Šå¤©" in date_str:
                return True
        
        dt = parse_date(date_str)
        return dt >= self.cutoff if dt else False

    # RSS íŒŒì„œ (ëŒ€ë§Œìš©)
    def parse_google_news_rss(self, raw_xml: str) -> list:
        results = []
        try:
            root = ET.fromstring(raw_xml.encode("utf-8"))
            for item in root.iter("item"):
                title = clean_text(item.findtext("title"))
                url = item.findtext("link")
                date = item.findtext("pubDate")
                if self.is_within_cutoff(date):
                    results.append({"title": title, "url": url, "date": date, "media": "Google News"})
        except: pass
        return results[:15]

    # ë°”ì´ë‘ íŒŒì„œ (ì¤‘êµ­ìš©)
    def parse_baidu_news(self, soup: BeautifulSoup) -> list:
        results = []
        for item in soup.select("div.result"):
            a_tag = item.select_one("h3.c-title a")
            if not a_tag: continue
            title = clean_text(a_tag.get_text())
            url = a_tag.get("href")
            date_tag = item.select_one("span.c-author")
            date_str = date_tag.get_text() if date_tag else ""
            if self.is_within_cutoff(date_str, is_china=True):
                results.append({"title": title, "url": url, "date": date_str, "media": "Baidu"})
        return results[:15]

    def crawl(self, category: str, keyword_ko: str):
        all_articles = []
        sources = SOURCES.get(category, [])
        for src in sources:
            # í‚¤ì›Œë“œ ë³€ì—­
            kw = KEYWORD_TRANSLATIONS.get(keyword_ko, {}).get(src["language"], keyword_ko)
            search_url = src["search_url"].format(keyword=quote(kw))
            
            try:
                resp = self.session.get(search_url, timeout=15)
                if src.get("parser") == "google_news_rss":
                    articles = self.parse_google_news_rss(resp.text)
                elif src.get("parser") == "baidu_news":
                    articles = self.parse_baidu_news(BeautifulSoup(resp.text, "html.parser"))
                else:
                    # ì¼ë°˜ ë§¤ì²´ ë²”ìš© íŒŒì„œ (ë‹¨ìˆœí™”)
                    articles = [] 
                
                for a in articles:
                    a.update({"source": src["name"], "flag": src["flag"], "lang": src["language"]})
                    all_articles.append(a)
                time.sleep(1)
            except: continue
        return all_articles

# â”€â”€â”€ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_integrated_pipeline(keyword: str, days: int = 7):
    crawler = IntegratedNewsCrawler(days=days)
    results = {}
    
    for region in ["china", "taiwan"]:
        print(f"--- {region} ìˆ˜ì§‘ ì‹œì‘ ---")
        articles = crawler.crawl(region, keyword)
        
        # í•œêµ­ì–´ ë²ˆì—­ ì¶”ê°€
        for a in articles:
            a["title_ko"] = translate_to_korean(a["title"], a["lang"])
        
        results[region] = articles
    
    return results

# ì‹¤í–‰ ì˜ˆì‹œ
# final_data = run_integrated_pipeline("ë¬´ì‹ ì‚¬", days=3)

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re

# ... (ê¸°ì¡´ HEADERS, KEYWORD_TRANSLATIONS ë™ì¼)

SOURCES = {
    "japan": [
        {
            "name": "Yahoo Japan News",
            "url": "https://news.yahoo.co.jp",
            # [ê°œì„ ] ê¸°ì‚¬ íƒ­ì—ì„œ ìµœì‹ ìˆœ(pub)ìœ¼ë¡œ ê²€ìƒ‰
            "search_url": "https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&sort=pub",
            "language": "ja", 
            "flag": "ğŸ‡¯ğŸ‡µ"
        },
    ],
    "china": [
        # ... (ê¸°ì¡´ ì¤‘êµ­ ì†ŒìŠ¤ ìœ ì§€)
    ],
    "taiwan": [
        # ... (ê¸°ì¡´ ëŒ€ë§Œ ì†ŒìŠ¤ ìœ ì§€)
    ],
}

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    # [ì‹ ê·œ] ì•¼í›„ ì¬íŒ¬ ì „ìš© ìƒëŒ€ ì‹œê°„ íŒŒì‹± (nì‹œê°„ ì „ ë“±)
    def parse_relative_date(self, text: str) -> datetime:
        now = datetime.now()
        if 'ë¶„ ì „' in text or 'åˆ†å‰' in text:
            m = re.search(r'(\d+)', text)
            return now - timedelta(minutes=int(m.group(1))) if m else now
        if 'ì‹œê°„ ì „' in text or 'æ™‚é–“å‰' in text:
            m = re.search(r'(\d+)', text)
            return now - timedelta(hours=int(m.group(1))) if m else now
        if 'ì–´ì œ' in text or 'æ˜¨æ—¥' in text:
            return now - timedelta(days=1)
        
        # ì¼ë°˜ ë‚ ì§œ íŒ¨í„´ ì‹œë„
        parsed = parse_date(text)
        return parsed if parsed else now

    def is_within_cutoff(self, date_str: str) -> bool:
        dt = self.parse_relative_date(date_str)
        return dt >= self.cutoff

    def parse_yahoo_japan(self, soup: BeautifulSoup) -> list[dict]:
        """ì•¼í›„ ì¬íŒ¬ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì „ìš© íŒŒì„œ"""
        results = []
        # ì•¼í›„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì¹´ë“œ ì…€ë ‰í„°
        items = soup.select('li.sw-Card') 
        for item in items:
            title_tag = item.select_one('h3.sw-Card__title')
            a_tag = item.select_one('a.sw-Card__titleInner')
            date_tag = item.select_one('span.sw-Card__time')
            source_tag = item.select_one('span.sw-Card__sender') # ì›ë¬¸ ë§¤ì²´ëª… (WWD, Fashionsnap ë“±)

            if title_tag and a_tag:
                title = clean_text(title_tag.get_text())
                url = a_tag['href']
                date_text = date_tag.get_text() if date_tag else ""
                source_name = source_tag.get_text() if source_tag else "Yahoo News"

                if self.is_within_cutoff(date_text):
                    results.append({
                        "title": title,
                        "url": url,
                        "date": date_text,
                        "source": source_name, # ì•¼í›„ ë‚´ ì‹¤ì œ ì¶œì²˜ í‘œì‹œ
                        "flag": "ğŸ‡¯ğŸ‡µ",
                        "language": "ja"
                    })
        return results

    def search_source(self, source: dict, keyword: str) -> list[dict]:
        print(f"  ê²€ìƒ‰ ì‹œë„: {source['name']} ({keyword})")
        soup = self.fetch(source["search_url"].format(keyword=quote(keyword)))
        if not soup: return []

        # ì¼ë³¸(ì•¼í›„)ì¸ ê²½ìš° ì „ìš© íŒŒì„œ ì‚¬ìš©, ë‚˜ë¨¸ì§€ëŠ” ë²”ìš© íŒŒì„œ ì‚¬ìš©
        if source["name"] == "Yahoo Japan News":
            return self.parse_yahoo_japan(soup)
        
        results = self.parse_generic(soup, source["url"])
        for r in results:
            r.update({"source": source["name"], "source_url": source["url"],
                       "language": source["language"], "flag": source.get("flag", "")})
        return results

# ... (ì´í•˜ translate_articles, run_pipeline ë“± ê¸°ì¡´ ë¡œì§ ìœ ì§€)

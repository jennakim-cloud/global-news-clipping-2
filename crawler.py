"""
crawler.py - ë‰´ìŠ¤ ìˆ˜ì§‘ & Google ë²ˆì—­ ëª¨ë“ˆ

[ì¼ë³¸] Google News RSS (hl=ja, gl=JP)
[ì¤‘êµ­] ë°”ì´ë‘ ê²€ìƒ‰ ì—”ì§„ + ê°œë³„ ë§¤ì²´ ë³‘í–‰ ìˆ˜ì§‘
[ëŒ€ë§Œ] Google News RSS (hl=zh-TW, gl=TW)
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re
import xml.etree.ElementTree as ET

# â”€â”€â”€ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€ í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",              "zh": "MUSINSA",    "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",       "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",          "zh": "Kç¾å¦†",      "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",                    "zh": "ç”µå•†",       "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",            "zh": "æ—¶å°š",       "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",               "zh": "é›¶å”®",       "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",            "zh": "ç¾å¦",       "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",                   "zh": "SPA",        "tw": "SPA"},
    "íŒ¨ì…˜ ë¸Œëœë“œ": {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰",    "zh": "æ—¶å°šå“ç‰Œ",   "tw": "æ™‚å°šå“ç‰Œ"},
    "ìœ ë‹ˆí´ë¡œ":    {"ja": "ãƒ¦ãƒ‹ã‚¯ãƒ­",               "zh": "ä¼˜è¡£åº“",     "tw": "Uniqlo"},
    "ë¬´ì¸ì–‘í’ˆ":    {"ja": "ç„¡å°è‰¯å“",               "zh": "æ— å°è‰¯å“",   "tw": "ç„¡å°è‰¯å“"},
    "ì•ˆíƒ€":        {"ja": "ã‚¢ãƒ³ã‚¿",                 "zh": "å®‰è¸",       "tw": "å®‰è¸"},
    # ì¶”ê°€ëœ í‚¤ì›Œë“œ
    "í•œêµ­ ë¸Œëœë“œ": {"ja": "éŸ“å›½ãƒ–ãƒ©ãƒ³ãƒ‰",           "zh": "éŸ©å›½å“ç‰Œ",   "tw": "éŸ“åœ‹å“ç‰Œ"},
    "í•œêµ­":        {"ja": "éŸ“å›½",                   "zh": "éŸ©å›½",       "tw": "éŸ“åœ‹"},
    "í•œêµ­ë°œ":      {"ja": "éŸ“å›½ç™º",                 "zh": "æºìéŸ©å›½",   "tw": "æºè‡ªéŸ“åœ‹"}
}

# â”€â”€â”€ ë§¤ì²´ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    "japan": [
        {
            "name": "Google News (æ—¥æœ¬)",
            "url": "https://news.google.com",
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja",
            "language": "ja",
            "flag": "ğŸ‡¯ğŸ‡µ",
            "parser": "google_news_rss",
        },
    ],
    "china": [
        # â”€â”€ ê²€ìƒ‰ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        {
            "name": "ç™¾åº¦æ–°é—»",
            "url": "https://news.baidu.com",
            "search_url": "https://news.baidu.com/ns?word={keyword}&tn=news&from=news&ie=utf-8&rn=20",
            "language": "zh", "flag": "ğŸ‡¨ğŸ‡³", "parser": "baidu_news",
        },
        {
            "name": "æœç‹—æ–°é—»",
            "url": "https://news.sogou.com",
            "search_url": "https://news.sogou.com/news?query={keyword}&ie=utf8",
            "language": "zh", "flag": "ğŸ‡¨ğŸ‡³", "parser": "sogou_news",
        },
        # â”€â”€ ê°œë³„ ë§¤ì²´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        {"name": "ç•Œé¢æ–°é—»",          "url": "https://www.jiemian.com",     "search_url": "https://www.jiemian.com/search.html?keywords={keyword}",    "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª",              "url": "https://36kr.com",            "search_url": "https://36kr.com/search/articles/{keyword}",                "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "äº¿é‚¦åŠ¨åŠ›",          "url": "https://www.ebrun.com",       "search_url": "https://www.ebrun.com/search/?q={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ–°æµª",              "url": "https://www.sina.com.cn",     "search_url": "https://search.sina.com.cn/?q={keyword}&range=all&c=news",  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co",           "url": "https://luxe.co",             "search_url": "https://luxe.co/?s={keyword}",                              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD Greater China", "url": "https://wwdgreaterchina.com", "search_url": "https://wwdgreaterchina.com/?s={keyword}",                  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Vogue China",       "url": "https://www.vogue.com.cn",    "search_url": "https://www.vogue.com.cn/search?q={keyword}",               "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "ç¬¬ä¸€è´¢ç»",          "url": "https://www.yicai.com",       "search_url": "https://www.yicai.com/search/?keys={keyword}",              "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "èµ¢å•†ç½‘",            "url": "https://m.winshang.com",      "search_url": "https://m.winshang.com/search.html?keyword={keyword}",      "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æ¾æ¹ƒæ–°é—»",          "url": "https://www.thepaper.cn",     "search_url": "https://www.thepaper.cn/search/?keyword={keyword}",         "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "æœç‹æ–°é—»",          "url": "https://search.sohu.com",     "search_url": "https://search.sohu.com/?smarter=true&query={keyword}&type=news", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Concall",           "url": "https://cn.concall.com",      "search_url": "https://cn.concall.com/search?q={keyword}",                 "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Ulife Media",       "url": "https://www.ulife-media.com", "search_url": "https://www.ulife-media.com/?s={keyword}",                  "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        {
            "name": "Google News (å°ç£)",
            "url": "https://news.google.com",
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
            "language": "tw",
            "flag": "ğŸ‡¹ğŸ‡¼",
            "parser": "google_news_rss",
        },
    ],
}

# â”€â”€â”€ ë‚ ì§œ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RFC2822_PATTERN = re.compile(
    r"(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})\s+(\d{2}:\d{2}:\d{2})"
)
MONTH_MAP = {
    "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4,  "May": 5,  "Jun": 6,
    "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12,
}

DATE_PATTERNS = [
    (r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", "%Y-%m-%dT%H:%M:%S"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}", "%Y-%m-%d %H:%M:%S"),
    (r"\d{4}/\d{2}/\d{2} \d{2}:\d{2}",        "%Y/%m/%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}",        "%Y-%m-%d %H:%M"),
    (r"\d{4}-\d{2}-\d{2}",                    "%Y-%m-%d"),
    (r"\d{4}/\d{2}/\d{2}",                    "%Y/%m/%d"),
    (r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",            "%Yå¹´%mæœˆ%dæ—¥"),
]


def parse_date(text: str):
    """RFC 2822 / ISO 8601 / ì¼ë°˜ ë‚ ì§œ í˜•ì‹ íŒŒì‹±. ì‹¤íŒ¨ ì‹œ None."""
    if not text:
        return None
    text = text.strip()

    m = RFC2822_PATTERN.search(text)
    if m:
        day, mon_str, year, time_str = m.groups()
        try:
            return datetime(int(year), MONTH_MAP[mon_str], int(day),
                            *map(int, time_str.split(":")))
        except (ValueError, KeyError):
            pass

    text_clean = re.sub(r"[+Z]\d{2}:?\d{0,2}$", "", text)
    for pattern, fmt in DATE_PATTERNS:
        m = re.search(pattern, text_clean)
        if m:
            try:
                return datetime.strptime(m.group(0), fmt)
            except ValueError:
                continue

    return None


def clean_text(text: str) -> str:
    """HTML íƒœê·¸Â·ì—”í‹°í‹°Â·ê³¼ë„í•œ ê³µë°± ì œê±°"""
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = (text.replace("&amp;", "&").replace("&lt;", "<")
                .replace("&gt;", ">").replace("&nbsp;", " ").replace("&quot;", '"'))
    return re.sub(r"\s+", " ", text).strip()


# â”€â”€â”€ Google ë²ˆì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text or not text.strip():
        return text
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS,
        )
        resp.raise_for_status()
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or text
    except Exception:
        return text


def translate_keyword_to_lang(keyword_ko: str, lang: str) -> str:
    """í‚¤ì›Œë“œë¥¼ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­ (ì‚¬ì „ ìš°ì„  â†’ Google ë²ˆì—­ fallback)"""
    if keyword_ko in KEYWORD_TRANSLATIONS:
        return KEYWORD_TRANSLATIONS[keyword_ko].get(lang, keyword_ko)
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": "ko", "tl": lang_map.get(lang, "ja"), "dt": "t", "q": keyword_ko},
            timeout=10, headers=HEADERS,
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip() or keyword_ko
    except Exception:
        return keyword_ko


# â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch_raw(self, url: str, timeout: int = 15):
        """ì›ì‹œ í…ìŠ¤íŠ¸ ë°˜í™˜ (RSS XML íŒŒì‹±ìš©)"""
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return resp.text
        except Exception:
            return None

    def fetch(self, url: str, timeout: int = 15):
        """BeautifulSoup ë°˜í™˜ (HTML í¬ë¡¤ë§ìš©)"""
        raw = self.fetch_raw(url, timeout)
        return BeautifulSoup(raw, "html.parser") if raw else None

    def is_within_cutoff(self, date_str: str) -> bool:
        """ë‚ ì§œë¥¼ ëª…í™•íˆ íŒŒì‹±í•œ ê²½ìš°ì—ë§Œ í¬í•¨ (ë‚ ì§œ ë¶ˆëª… â†’ False)"""
        dt = parse_date(date_str)
        if dt is None:
            return False
        return dt >= self.cutoff

    def is_within_cutoff_cn(self, date_str: str) -> bool:
        """
        ì¤‘êµ­ì–´ ìƒëŒ€ì‹œê°„(Nå°æ—¶å‰, Nåˆ†é’Ÿå‰, ä»Šå¤©, æ˜¨å¤©, åˆšåˆš) + ì ˆëŒ€ë‚ ì§œ ì²˜ë¦¬.
        ë‚ ì§œ ë¶ˆëª… â†’ False.
        """
        if not date_str:
            return False
        now = datetime.now()
        s = date_str.strip()

        if "åˆ†é’Ÿå‰" in s:
            m = re.search(r"(\d+)", s)
            return (now - timedelta(minutes=int(m.group(1)))) >= self.cutoff if m else True
        if "å°æ—¶å‰" in s:
            m = re.search(r"(\d+)", s)
            return (now - timedelta(hours=int(m.group(1)))) >= self.cutoff if m else True
        if "åˆšåˆš" in s:
            return True
        if "ä»Šå¤©" in s or "ä»Šæ—¥" in s:
            return True
        if "æ˜¨å¤©" in s or "æ˜¨æ—¥" in s:
            return (now - timedelta(days=1)) >= self.cutoff

        dt = parse_date(s)
        if dt is None:
            return False
        return dt >= self.cutoff

    # â”€â”€ Google News RSS íŒŒì„œ (ì¼ë³¸Â·ëŒ€ë§Œ ê³µìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_google_news_rss(self, raw_xml: str) -> list:
        """
        Google News RSS XML íŒŒì‹±.
        <item>: <title>ì œëª© - ë§¤ì²´ëª…</title> / <link> / <pubDate> / <source>
        ì¼ë³¸(hl=ja)Â·ëŒ€ë§Œ(hl=zh-TW) ëª¨ë‘ ë™ì¼ êµ¬ì¡°.
        """
        results = []
        try:
            root = ET.fromstring(raw_xml.encode("utf-8"))
        except ET.ParseError:
            return []

        for item in root.iter("item"):
            title_el = item.find("title")
            if title_el is None or not title_el.text:
                continue
            raw_title = clean_text(title_el.text)

            # "ì œëª© - ë§¤ì²´ëª…" ë¶„ë¦¬
            media_name = ""
            if " - " in raw_title:
                parts      = raw_title.rsplit(" - ", 1)
                title      = parts[0].strip()
                media_name = parts[1].strip()
            else:
                title = raw_title

            if len(title) < 5:
                continue

            # URL: <link> ìš°ì„ , ì—†ìœ¼ë©´ <guid>
            url = ""
            link_el = item.find("link")
            if link_el is not None and link_el.text:
                url = link_el.text.strip()
            if not url:
                guid_el = item.find("guid")
                if guid_el is not None and guid_el.text:
                    url = guid_el.text.strip()

            source_el = item.find("source")
            if source_el is not None and source_el.text:
                media_name = media_name or clean_text(source_el.text)

            pub_el   = item.find("pubDate")
            date_str = pub_el.text.strip() if pub_el is not None and pub_el.text else ""

            if not self.is_within_cutoff(date_str):
                continue

            results.append({"title": title, "url": url, "date": date_str, "media": media_name})

        return results[:20]

    # â”€â”€ ë°”ì´ë‘ ë‰´ìŠ¤ íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_baidu_news(self, soup: BeautifulSoup) -> list:
        """
        ç™¾åº¦æ–°é—» ê²€ìƒ‰ ê²°ê³¼ íŒŒì„œ.
        div.result > h3.c-title > a  +  span.c-author (ë‚ ì§œÂ·ë§¤ì²´)
        """
        results = []
        seen = set()

        containers = (
            soup.select("div.result") or
            soup.select("div[class*='result']") or
            soup.select("div.news-box") or
            soup.find_all("div", class_=re.compile(r"^result"))
        )

        for item in containers:
            title_tag = (
                item.select_one("h3.c-title > a") or
                item.select_one("h3 > a") or
                item.select_one("a.news-title") or
                item.find("h3")
            )
            if not title_tag:
                continue

            a_tag = title_tag if title_tag.name == "a" else title_tag.find("a", href=True)
            if not a_tag:
                continue

            title = clean_text(title_tag.get_text())
            url   = a_tag.get("href", "")
            if not url or len(title) < 5 or url in seen:
                continue
            seen.add(url)

            # ë‚ ì§œ íƒìƒ‰
            date_str = ""
            for sel in ["span.c-author", "p.c-author", "span[class*='time']",
                        "span[class*='date']", "cite"]:
                tag = item.select_one(sel)
                if tag:
                    candidate = clean_text(tag.get_text())
                    if self.is_within_cutoff_cn(candidate):
                        date_str = candidate
                        break

            if not date_str:
                raw = item.get_text(" ", strip=True)
                rel = re.search(r"(\d+å°æ—¶å‰|\d+åˆ†é’Ÿå‰|æ˜¨å¤©\s*\d+:\d+|ä»Šå¤©\s*\d+:\d+|åˆšåˆš)", raw)
                if rel:
                    date_str = rel.group(0)
                else:
                    for pattern, _ in DATE_PATTERNS:
                        m = re.search(pattern, raw)
                        if m:
                            date_str = m.group(0)
                            break

            if not self.is_within_cutoff_cn(date_str):
                continue

            media_tag = (
                item.select_one("span.c-author") or
                item.select_one("cite") or
                item.select_one("p.c-author")
            )
            media = clean_text(media_tag.get_text()).split()[0] if media_tag else ""

            results.append({"title": title, "url": url, "date": date_str, "media": media})

        return results[:20]


    # â”€â”€ ì†Œìš°ê±°ìš° ë‰´ìŠ¤ íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_sogou_news(self, soup: BeautifulSoup) -> list:
        """
        æœç‹—æ–°é—» ê²€ìƒ‰ ê²°ê³¼ íŒŒì„œ.
        â€» ê¸°ì‚¬ URLì´ ì†Œìš°ê±°ìš° íŠ¸ë˜í‚¹ URL(news.sogou.com/link?url=...)ì¼ ìˆ˜ ìˆìŒ.
        """
        results = []
        seen = set()

        containers = (
            soup.select("div.news-item") or
            soup.select("li.news-item") or
            soup.select("div.vrNews") or
            soup.select("div[class*='news']") or
            soup.find_all("div", class_=re.compile(r"item|result|news", re.I))
        )

        for item in containers:
            h_tag = item.find(["h3", "h2", "h4"])
            if not h_tag:
                continue
            a_tag = h_tag.find("a", href=True) or item.find("a", href=True)
            if not a_tag:
                continue

            title = clean_text(h_tag.get_text())
            url   = a_tag.get("href", "")
            if not url or len(title) < 5:
                continue
            if url.startswith("/"):
                url = "https://news.sogou.com" + url
            if url in seen:
                continue
            seen.add(url)

            # ë‚ ì§œ íƒìƒ‰
            date_str = ""
            for sel in ["span.time", "span.date", "span[class*='time']",
                        "span[class*='date']", "em.time", "i.time"]:
                tag = item.select_one(sel)
                if tag:
                    candidate = clean_text(tag.get_text())
                    if self.is_within_cutoff_cn(candidate):
                        date_str = candidate
                        break

            if not date_str:
                raw = item.get_text(" ", strip=True)
                rel = re.search(r"(\d+å°æ—¶å‰|\d+åˆ†é’Ÿå‰|æ˜¨å¤©\s*\d+:\d+|ä»Šå¤©\s*\d+:\d+|åˆšåˆš)", raw)
                if rel:
                    date_str = rel.group(0)
                else:
                    for pattern, _ in DATE_PATTERNS:
                        m = re.search(pattern, raw)
                        if m:
                            date_str = m.group(0)
                            break

            if not self.is_within_cutoff_cn(date_str):
                continue

            src_tag = (
                item.select_one("span.src") or
                item.select_one("a.src") or
                item.select_one("span[class*='source']") or
                item.select_one("cite")
            )
            media = clean_text(src_tag.get_text()) if src_tag else ""

            results.append({"title": title, "url": url, "date": date_str, "media": media})

        return results[:20]

    # â”€â”€ ë²”ìš© HTML íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _find_date_in_tag(self, tag) -> str:
        t = tag.find("time")
        if t:
            candidate = t.get("datetime") or t.get_text(strip=True)
            if parse_date(candidate):
                return candidate
        d = tag.find(True, class_=re.compile(r"\b(date|time|pub|posted|created|updated)\b", re.I))
        if d:
            candidate = d.get("datetime") or d.get_text(strip=True)
            if parse_date(candidate):
                return candidate
        raw = tag.get_text(" ", strip=True)
        for pattern, _ in DATE_PATTERNS:
            m = re.search(pattern, raw)
            if m:
                return m.group(0)
        return ""

    def parse_generic(self, soup: BeautifulSoup, base_url: str) -> list:
        candidates = []
        seen = set()

        for article in soup.find_all("article"):
            title_tag = article.find(["h1", "h2", "h3", "h4"])
            a_tag     = article.find("a", href=True)
            if not title_tag or not a_tag:
                continue
            title = clean_text(title_tag.get_text())
            url   = urljoin(base_url, a_tag["href"])
            date  = self._find_date_in_tag(article)
            if url not in seen and len(title) > 5:
                seen.add(url)
                candidates.append({"title": title, "url": url, "date": date})

        if not candidates:
            for h in soup.find_all(["h2", "h3"]):
                a_tag = h.find("a", href=True) or h.find_parent("a", href=True)
                if not a_tag:
                    continue
                title  = clean_text(h.get_text())
                url    = urljoin(base_url, a_tag["href"])
                parent = h.find_parent(["li", "div", "section"])
                date   = self._find_date_in_tag(parent) if parent else ""
                if url not in seen and len(title) > 5:
                    seen.add(url)
                    candidates.append({"title": title, "url": url, "date": date})

        return [c for c in candidates if self.is_within_cutoff(c["date"])][:12]

    # â”€â”€ ë§¤ì²´ ê²€ìƒ‰ (íŒŒì„œ ë¶„ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def search_source(self, source: dict, keyword: str) -> list:
        search_url  = source["search_url"].format(keyword=quote(keyword))
        parser_name = source.get("parser", "generic")

        if parser_name == "google_news_rss":
            raw     = self.fetch_raw(search_url)
            results = self.parse_google_news_rss(raw) if raw else []
        elif parser_name == "baidu_news":
            soup    = self.fetch(search_url)
            results = self.parse_baidu_news(soup) if soup else []
        elif parser_name == "sogou_news":
            soup    = self.fetch(search_url)
            results = self.parse_sogou_news(soup) if soup else []
        else:
            soup    = self.fetch(search_url)
            results = self.parse_generic(soup, source["url"]) if soup else []

        # URL íŒ¨í„´ í•„í„° (ì¹´í…Œê³ ë¦¬Â·íƒœê·¸ ë§í¬ ì œê±°)
        exclude_patterns   = source.get("exclude_url_patterns", [])
        exclude_exact_urls = set(source.get("exclude_exact_urls", []))
        if exclude_patterns or exclude_exact_urls:
            before  = len(results)
            results = [
                r for r in results
                if r.get("url", "") not in exclude_exact_urls
                and not any(p in r.get("url", "") for p in exclude_patterns)
            ]
            if before - len(results):
                print(f"  [{source['name']}] ë¹„ê¸°ì‚¬ ë§í¬ {before - len(results)}ê±´ ì œì™¸")

        # ê³µí†µ ë©”íƒ€ ì„¸íŒ…
        for r in results:
            r.setdefault("source",     source["name"])
            r.setdefault("source_url", source["url"])
            r.setdefault("language",   source["language"])
            r.setdefault("flag",       source.get("flag", ""))

        return results

    def crawl_category(self, category: str, keyword_ko: str, progress_callback=None) -> list:
        all_articles = []
        for source in SOURCES.get(category, []):
            keyword = translate_keyword_to_lang(keyword_ko, source["language"])
            if progress_callback:
                progress_callback(source["name"], keyword)
            all_articles.extend(self.search_source(source, keyword))
            time.sleep(0.8)
        return all_articles


# â”€â”€â”€ ë²ˆì—­ ì¼ê´„ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_articles(articles: list, progress_callback=None) -> list:
    lang_map = {"ja": "ja", "zh": "zh-CN", "tw": "zh-TW"}
    for i, a in enumerate(articles):
        if progress_callback:
            progress_callback(i + 1, len(articles), a.get("source", ""))
        src = lang_map.get(a.get("language", "ja"), "auto")
        a["title_ko"] = translate_to_korean(a["title"], src_lang=src)
        time.sleep(0.3)
    return articles


# â”€â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    keyword_ko: str,
    days: int = 7,
    active_categories: list = None,
    on_status=None,
    on_progress=None,
) -> dict:
    """
    ìˆ˜ì§‘ â†’ ì¤‘ë³µì œê±° â†’ ë²ˆì—­ íŒŒì´í”„ë¼ì¸.
    ë°˜í™˜ê°’: {"japan": [...], "china": [...], "taiwan": [...], "meta": {...}}
    """
    if active_categories is None:
        active_categories = ["japan", "china", "taiwan"]

    def _s(msg):
        if on_status: on_status(msg)

    def _p(val, text=""):
        if on_progress: on_progress(min(val, 1.0), text)

    crawler       = NewsCrawler(days=days)
    collected     = {cat: [] for cat in ["japan", "china", "taiwan"]}
    total_sources = sum(len(SOURCES[c]) for c in active_categories)
    done          = 0

    for cat in active_categories:
        label = {"japan": "ğŸ‡¯ğŸ‡µ ì¼ë³¸", "china": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­", "taiwan": "ğŸ‡¹ğŸ‡¼ ëŒ€ë§Œ"}[cat]
        _s(f"{label} ë§¤ì²´ ìˆ˜ì§‘ ì¤‘...")

        def _cb(name, kw, _label=label):
            nonlocal done
            done += 1
            _p(done / total_sources * 0.6, f"{_label} Â· {name} ({kw})")

        collected[cat] = crawler.crawl_category(cat, keyword_ko, _cb)

        # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ê²€ìƒ‰ ì—”ì§„ + ê°œë³„ ë§¤ì²´ ì¤‘ë³µ ë°©ì§€)
        seen_urls = set()
        deduped   = []
        for a in collected[cat]:
            if a["url"] not in seen_urls:
                seen_urls.add(a["url"])
                deduped.append(a)
        collected[cat] = deduped

    # ë²ˆì—­
    all_t = collected["japan"] + collected["china"] + collected["taiwan"]
    _s(f"Google ë²ˆì—­ ì²˜ë¦¬ ì¤‘ (ì´ {len(all_t)}ê±´)...")

    def _tcb(cur, total, source):
        _p(0.6 + (cur / max(total, 1)) * 0.4, f"ë²ˆì—­ ì¤‘ {cur}/{total} Â· {source}")

    translate_articles(all_t, _tcb)
    _p(1.0, "ì™„ë£Œ!")

    return {
        "japan":  collected["japan"],
        "china":  collected["china"],
        "taiwan": collected["taiwan"],
        "meta": {
            "keyword":      keyword_ko,
            "days":         days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M"),
        },
    }

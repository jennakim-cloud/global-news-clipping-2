"""
crawler.py - ì•¼í›„ ìž¬íŒ¬ ë‰´ìŠ¤ í†µí•© ê²€ìƒ‰ ë° Google ë²ˆì—­ ëª¨ë“ˆ
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# ê¸°ì¡´ í‚¤ì›Œë“œ ë§µí•‘ ìœ ì§€
KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",          "zh": "MUSINSA",   "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",   "zh": "Kç¾Žå¦†",     "tw": "Kç¾Žå¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",              "zh": "ç”µå•†",      "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",     "zh": "æ—¶å°š",      "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",        "zh": "é›¶å”®",      "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",     "zh": "ç¾Žå¦†",      "tw": "ç¾Žå¦"},
    "SPA":         {"ja": "SPA",             "zh": "SPA",       "tw": "SPA"},
    "ëŸ­ì…”ë¦¬":      {"ja": "ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼",  "zh": "å¥¢ä¾ˆå“",    "tw": "å¥¢ä¾ˆå“"},
    "ì§€ì†ê°€ëŠ¥ì„±":  {"ja": "ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£", "zh": "å¯æŒç»­å‘å±•", "tw": "æ°¸çºŒç™¼å±•"},
}

# ë§¤ì²´ ì„¤ì • (ì¼ë³¸ì€ ì•¼í›„ ë‰´ìŠ¤ë¡œ í†µí•©)
SOURCES = {
    "japan": [
        {
            "name": "Yahoo Japan News", 
            "url": "https://news.yahoo.co.jp", 
            "search_url": "https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&sort=pub", 
            "language": "ja", 
            "flag": "ðŸ‡¯ðŸ‡µ"
        },
    ],
    "china": [
        {"name": "ç•Œé¢æ–°é—»", "url": "https://www.jiemian.com", "search_url": "https://www.jiemian.com/search.html?keywords={keyword}", "language": "zh", "flag": "ðŸ‡¨ðŸ‡³"},
        {"name": "36æ°ª", "url": "https://36kr.com", "search_url": "https://36kr.com/search/articles/{keyword}", "language": "zh", "flag": "ðŸ‡¨ðŸ‡³"},
        {"name": "ç¬¬ä¸€è´¢ç»", "url": "https://www.yicai.com", "search_url": "https://www.yicai.com/search/?keys={keyword}", "language": "zh", "flag": "ðŸ‡¨ðŸ‡³"},
        {"name": "Luxe.co", "url": "https://luxe.co", "search_url": "https://luxe.co/?s={keyword}", "language": "zh", "flag": "ðŸ‡¨ðŸ‡³"},
    ],
    "taiwan": [
        {"name": "æ•¸ä½æ™‚ä»£", "url": "https://www.bnext.com.tw", "search_url": "https://www.bnext.com.tw/search/{keyword}", "language": "tw", "flag": "ðŸ‡¹ðŸ‡¼"},
        {"name": "å·¥å•†æ™‚å ±", "url": "https://www.ctee.com.tw", "search_url": "https://www.ctee.com.tw/search?q={keyword}", "language": "tw", "flag": "ðŸ‡¹ðŸ‡¼"},
    ],
}

# ë‚ ì§œ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
def parse_date(text: str):
    if not text: return None
    text = text.strip()
    now = datetime.now()
    # ì•¼í›„ ìž¬íŒ¬ ì „ìš© ìƒëŒ€ ì‹œê°„ ì²˜ë¦¬
    if 'åˆ†å‰' in text:
        m = re.search(r'(\d+)', text)
        return now - timedelta(minutes=int(m.group(1))) if m else now
    if 'æ™‚é–“å‰' in text:
        m = re.search(r'(\d+)', text)
        return now - timedelta(hours=int(m.group(1))) if m else now
    if 'æ˜¨æ—¥' in text:
        return now - timedelta(days=1)
    
    patterns = [
        (r"\d{4}-\d{2}-\d{2}", "%Y-%m-%d"),
        (r"\d{4}/\d{2}/\d{2}", "%Y/%m/%d"),
        (r"\d{4}å¹´\d{1,2}ì›”\d{1,2}ì¼", "%Yå¹´%mì›”%dì¼"),
    ]
    for pattern, fmt in patterns:
        m = re.search(pattern, text)
        if m:
            try: return datetime.strptime(m.group(0), fmt)
            except: continue
    return None

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r"<[^>]+>", " ", text)
    return re.sub(r"\s+", " ", text).strip()

# Google ë²ˆì—­ API
def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text or not text.strip(): return text
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0]).strip()
    except: return text

class NewsCrawler:
    def __init__(self, days: int = 7):
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

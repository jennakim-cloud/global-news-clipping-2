import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import time
import re
import xml.etree.ElementTree as ET

# â”€â”€â”€ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,ja;q=0.6,zh-CN;q=0.5",
}

# â”€â”€â”€ í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORD_TRANSLATIONS = {
    "ë¬´ì‹ ì‚¬":      {"ja": "ãƒ ã‚·ãƒ³ã‚µ",              "zh": "MUSINSA",    "tw": "MUSINSA"},
    "í•œêµ­ íŒ¨ì…˜":   {"ja": "éŸ“å›½ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",       "zh": "éŸ©å›½æ—¶å°š",   "tw": "éŸ“åœ‹æ™‚å°š"},
    "K-ë·°í‹°":      {"ja": "Kãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",          "zh": "Kç¾å¦†",      "tw": "Kç¾å¦"},
    "ì´ì»¤ë¨¸ìŠ¤":    {"ja": "EC",                    "zh": "ç”µå•†",       "tw": "é›»å•†"},
    "íŒ¨ì…˜":        {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",            "zh": "æ—¶å°š",       "tw": "æ™‚å°š"},
    "ë¦¬í…Œì¼":      {"ja": "ãƒªãƒ†ãƒ¼ãƒ«",               "zh": "é›¶å”®",       "tw": "é›¶å”®"},
    "ë·°í‹°":        {"ja": "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",            "zh": "ç¾å¦",       "tw": "ç¾å¦"},
    "SPA":         {"ja": "SPA",                   "zh": "SPA",        "tw": "SPA"},
    "íŒ¨ì…˜ ë¸Œëœë“œ": {"ja": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ–ãƒ©ãƒ³ãƒ‰",    "zh": "æ—¶å°šå“ç‰Œ",   "tw": "æ™‚å°šå“ç‰Œ"},
    "ìœ ë‹ˆí´ë¡œ":    {"ja": "ãƒ¦ãƒ‹ã‚¯ãƒ­",               "zh": "ä¼˜è¡£åº“",     "tw": "Uniqlo"},
    "ë¬´ì¸ì–‘í’ˆ":    {"ja": "ç„¡å°è‰¯å“",               "zh": "æ— å°è‰¯å“",   "tw": "ç„¡å°è‰¯å“"},
    "ì•ˆíƒ€":        {"ja": "ã‚¢ãƒ³íƒ€",                 "zh": "å®‰è¸",       "tw": "å®‰è¸"},
    "í•œêµ­ ë¸Œëœë“œ": {"ja": "éŸ“å›½ãƒ–ãƒ©ãƒ³ãƒ‰",           "zh": "éŸ©å›½å“ç‰Œ",   "tw": "éŸ“åœ‹å“ç‰Œ"},
    "í•œêµ­":        {"ja": "éŸ“å›½",                   "zh": "éŸ©å›½",       "tw": "éŸ“åœ‹"},
    "í•œêµ­ë°œ":      {"ja": "éŸ“å›½ç™º",                 "zh": "æºè‡ªéŸ©å›½",   "tw": "æºè‡ªéŸ“åœ‹"}
}

# â”€â”€â”€ ë§¤ì²´ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCES = {
    "japan": [
        {
            "name": "Google News (æ—¥æœ¬)",
            "url": "https://news.google.com",
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja",
            "language": "ja", "flag": "ğŸ‡¯ğŸ‡µ", "parser": "google_news_rss",
        },
    ],
    "china": [
        {
            "name": "ç™¾åº¦æ–°é—»",
            "url": "https://news.baidu.com",
            "search_url": "https://news.baidu.com/ns?word={keyword}&tn=news&from=news&ie=utf-8&rn=20",
            "language": "zh", "flag": "ğŸ‡¨ğŸ‡³", "parser": "baidu_news",
        },
        {"name": "ç•Œé¢æ–°é—»", "url": "https://www.jiemian.com", "search_url": "https://www.jiemian.com/search.html?keywords={keyword}", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "36æ°ª", "url": "https://36kr.com", "search_url": "https://36kr.com/search/articles/{keyword}", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "Luxe.co", "url": "https://luxe.co", "search_url": "https://luxe.co/?s={keyword}", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
        {"name": "WWD China", "url": "https://wwdgreaterchina.com", "search_url": "https://wwdgreaterchina.com/?s={keyword}", "language": "zh", "flag": "ğŸ‡¨ğŸ‡³"},
    ],
    "taiwan": [
        {
            "name": "Google News (å°ç£)",
            "url": "https://news.google.com",
            "search_url": "https://news.google.com/rss/search?q={keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
            "language": "tw", "flag": "ğŸ‡¹ğŸ‡¼", "parser": "google_news_rss",
        },
    ],
}

# â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r"<[^>]+>", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def translate_to_korean(text: str, src_lang: str = "auto") -> str:
    if not text: return ""
    try:
        resp = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={"client": "gtx", "sl": src_lang, "tl": "ko", "dt": "t", "q": text},
            timeout=10, headers=HEADERS
        )
        return "".join(seg[0] for seg in resp.json()[0] if seg[0])
    except:
        return text

# â”€â”€â”€ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NewsCrawler:
    def __init__(self, days: int = 7):
        self.days = days
        self.cutoff = datetime.now() - timedelta(days=days)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def is_within_cutoff(self, date_str: str) -> bool:
        if not date_str: return False
        # ì¤‘êµ­ì–´ ìƒëŒ€ ì‹œê°„ ì²˜ë¦¬
        now = datetime.now()
        if "å°æ—¶å‰" in date_str:
            h = int(re.search(r"(\d+)", date_str).group(1))
            return (now - timedelta(hours=h)) >= self.cutoff
        if any(x in date_str for x in ["åˆ†é’Ÿå‰", "åˆšåˆš", "ä»Šå¤©", "ä»Šæ—¥"]):
            return True
        
        # ì¼ë°˜ ë‚ ì§œ íŒŒì‹± ì‹œë„
        try:
            clean_date = re.search(r"\d{4}[-/ë…„]\d{1,2}[-/ì›”]\d{1,2}", date_str).group(0)
            fmt = "%Y-%m-%d" if "-" in clean_date else "%Y/%m/%d"
            dt = datetime.strptime(clean_date.replace("ë…„","-").replace("ì›”","-").replace("ì¼",""), "%Y-%m-%d")
            return dt >= self.cutoff
        except:
            return True # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í¬í•¨

    def parse_google_news_rss(self, raw_xml: str) -> list:
        results = []
        try:
            root = ET.fromstring(raw_xml.encode("utf-8"))
            for item in root.iter("item"):
                title = clean_text(item.findtext("title"))
                url = item.findtext("link")
                date = item.findtext("pubDate")
                results.append({"title": title, "url": url, "date": date})
        except: pass
        return results

    def parse_baidu_news(self, soup: BeautifulSoup) -> list:
        results = []
        for item in soup.select("div.result"):
            a = item.select_one("h3.c-title a")
            author = item.select_one("span.c-author")
            if a:
                results.append({
                    "title": clean_text(a.get_text()),
                    "url": a.get("href"),
                    "date": author.get_text() if author else "",
                    "media": author.get_text().split()[0] if author else "Baidu"
                })
        return results

    def search_source(self, source: dict, keyword: str) -> list:
        kw_translated = KEYWORD_TRANSLATIONS.get(keyword, {}).get(source["language"], keyword)
        url = source["search_url"].format(keyword=quote(kw_translated))
        try:
            resp = self.session.get(url, timeout=15)
            if source.get("parser") == "google_news_rss":
                articles = self.parse_google_news_rss(resp.text)
            elif source.get("parser") == "baidu_news":
                articles = self.parse_baidu_news(BeautifulSoup(resp.text, "html.parser"))
            else:
                articles = [] # ê°œë³„ ë§¤ì²´ ë¡œì§ì€ ë²”ìš© íŒŒì„œ í•„ìš” ì‹œ ì¶”ê°€
            
            # ë©”íƒ€ë°ì´í„° ì£¼ì… ë° ë‚ ì§œ í•„í„°ë§
            valid = []
            for a in articles:
                if self.is_within_cutoff(a.get("date", "")):
                    a.update({"source": source["name"], "flag": source["flag"], "language": source["language"]})
                    valid.append(a)
            return valid
        except:
            return []

# â”€â”€â”€ íŒŒì´í”„ë¼ì¸ (app.py ì—°ê²°ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_pipeline(keyword_ko: str, days: int = 7, active_categories: list = None, on_status=None, on_progress=None) -> dict:
    crawler = NewsCrawler(days=days)
    if active_categories is None: active_categories = ["japan", "china", "taiwan"]
    
    collected = {cat: [] for cat in ["japan", "china", "taiwan"]}
    total_steps = sum(len(SOURCES[c]) for c in active_categories)
    current_step = 0

    for cat in active_categories:
        if on_status: on_status(f"{cat.upper()} ìˆ˜ì§‘ ì¤‘...")
        for src in SOURCES[cat]:
            current_step += 1
            if on_progress: on_progress(current_step / total_steps * 0.7, f"{src['name']} ìˆ˜ì§‘ ì¤‘")
            collected[cat].extend(crawler.search_source(src, keyword_ko))
            time.sleep(0.5)

    # ì¤‘ë³µ ì œê±° ë° ë²ˆì—­
    all_articles = []
    for cat in active_categories:
        seen = set()
        deduped = []
        for a in collected[cat]:
            if a["url"] not in seen:
                seen.add(a["url"])
                deduped.append(a)
                all_articles.append(a)
        collected[cat] = deduped

    if on_status: on_status("ë²ˆì—­ ì²˜ë¦¬ ì¤‘...")
    for i, a in enumerate(all_articles):
        if on_progress: on_progress(0.7 + (i / len(all_articles) * 0.3), f"ë²ˆì—­ ì¤‘: {a['source']}")
        src_lang = "ja" if a["language"] == "ja" else ("zh-CN" if a["language"] == "zh" else "zh-TW")
        a["title_ko"] = translate_to_korean(a["title"], src_lang)
        time.sleep(0.2)

    if on_progress: on_progress(1.0, "ìˆ˜ì§‘ ì™„ë£Œ")
    
    return {
        **collected,
        "meta": {
            "keyword": keyword_ko,
            "days": days,
            "generated_at": datetime.now().strftime("%Y.%m.%d %H:%M")
        }
    }
